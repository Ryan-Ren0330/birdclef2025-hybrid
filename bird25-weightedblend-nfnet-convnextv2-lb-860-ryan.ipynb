{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5acfda12",
   "metadata": {
    "papermill": {
     "duration": 0.011535,
     "end_time": "2025-05-27T03:16:18.943155",
     "exception": false,
     "start_time": "2025-05-27T03:16:18.931620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Only Submission(LoadLocalTrainModel)\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de5347",
   "metadata": {
    "papermill": {
     "duration": 0.007668,
     "end_time": "2025-05-27T03:16:18.959538",
     "exception": false,
     "start_time": "2025-05-27T03:16:18.951870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **ℹ️INFO**\n",
    "* This notebook is an weighted blend(nfnet * 0.6 + convnextv2 * 0.4).\n",
    "    * **GreatWork LB.850(nfnet)** https://www.kaggle.com/code/myso1987/post-processing-with-power-adjustment-for-low-rank\n",
    "    * **MyLocalTrainModel(convnextv2)** https://www.kaggle.com/datasets/hideyukizushi/bird25-d-330v2-ppv15-convnextv2-nano/data\n",
    "\n",
    "### **ℹ️WeightedBlend**\n",
    "* In CV tasks, it is common to adopt diverse backbones to ensure robustness, and I am sharing this in this competition because it is connected to the LB boost.\n",
    "* P.S. Adjusting the blending weights should make it fit LB better and improve the score. In that case, it is redundant, so I recommend submitting in a private notebook rather than a public notebook.\n",
    "\n",
    "### **ℹ️Appendix**\n",
    "* My old LB.829 notebook from this competition\n",
    "* https://www.kaggle.com/code/hideyukizushi/bird25-onlyinf-v2-s-focallossbce-cv-962-lb-829\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f61b92",
   "metadata": {
    "papermill": {
     "duration": 0.007728,
     "end_time": "2025-05-27T03:16:18.975053",
     "exception": false,
     "start_time": "2025-05-27T03:16:18.967325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Submission1(convnextv2)》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0b6f67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:16:18.992215Z",
     "iopub.status.busy": "2025-05-27T03:16:18.991839Z",
     "iopub.status.idle": "2025-05-27T03:16:18.997614Z",
     "shell.execute_reply": "2025-05-27T03:16:18.996561Z"
    },
    "papermill": {
     "duration": 0.016353,
     "end_time": "2025-05-27T03:16:18.999240",
     "exception": false,
     "start_time": "2025-05-27T03:16:18.982887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "# import warnings\n",
    "# import logging\n",
    "# import time\n",
    "# import math\n",
    "# import cv2\n",
    "# from pathlib import Path\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import librosa\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import timm\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# logging.basicConfig(level=logging.ERROR)\n",
    "# print(\"Finished import group000000000 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7feb695a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:16:19.016798Z",
     "iopub.status.busy": "2025-05-27T03:16:19.016274Z",
     "iopub.status.idle": "2025-05-27T03:16:19.020841Z",
     "shell.execute_reply": "2025-05-27T03:16:19.019842Z"
    },
    "papermill": {
     "duration": 0.015115,
     "end_time": "2025-05-27T03:16:19.022622",
     "exception": false,
     "start_time": "2025-05-27T03:16:19.007507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CFG:\n",
    " \n",
    "#     test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "#     submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "#     taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    \n",
    "#     # ------------------------------------------- #\n",
    "#     # [IMPORTANT]\n",
    "#     # * Melspectrogram & Audio Params\n",
    "#     # ------------------------------------------- #\n",
    "#     FS = 32000  \n",
    "#     WINDOW_SIZE = 5\n",
    "#     N_FFT = 2048\n",
    "#     HOP_LENGTH = 512\n",
    "#     N_MELS = 512\n",
    "#     FMIN = 20\n",
    "#     FMAX = 16000\n",
    "#     TARGET_SHAPE = (256, 256)\n",
    "\n",
    "#     # ------------------------------------------- #\n",
    "#     # * Model def\n",
    "#     # ------------------------------------------- #\n",
    "#     model_path = '/kaggle/input/bird25-d-330v2-ppv15-convnextv2-nano'\n",
    "#     model_name = 'convnextv2_nano.fcmae_ft_in22k_in1k'\n",
    "#     use_specific_folds = True\n",
    "#     folds = [0,1]\n",
    "#     in_channels = 1\n",
    "#     device = 'cpu'  \n",
    "    \n",
    "#     # Inference parameters\n",
    "#     batch_size = 16\n",
    "#     use_tta = False  \n",
    "#     tta_count = 3\n",
    "#     threshold = 0.5\n",
    "\n",
    "#     # util\n",
    "#     debug = False\n",
    "#     debug_count = 3\n",
    "\n",
    "# cfg = CFG()\n",
    "\n",
    "# print(f\"Using device: {cfg.device}\")\n",
    "# print(f\"Loading taxonomy data...\")\n",
    "# taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "# species_ids = taxonomy_df['primary_label'].tolist()\n",
    "# num_classes = len(species_ids)\n",
    "# print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97384fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:16:19.040101Z",
     "iopub.status.busy": "2025-05-27T03:16:19.039765Z",
     "iopub.status.idle": "2025-05-27T03:16:19.044368Z",
     "shell.execute_reply": "2025-05-27T03:16:19.043379Z"
    },
    "papermill": {
     "duration": 0.015199,
     "end_time": "2025-05-27T03:16:19.045953",
     "exception": false,
     "start_time": "2025-05-27T03:16:19.030754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class GeM(nn.Module):\n",
    "#     def __init__(self, p=3, eps=1e-6):\n",
    "#         super(GeM, self).__init__()\n",
    "#         self.p = nn.Parameter(torch.ones(1)*p)\n",
    "#         self.eps = eps\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "#     def gem(self, x, p=3, eps=1e-6):\n",
    "#         return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + \\\n",
    "#                 '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "#                 ', ' + 'eps=' + str(self.eps) + ')'\n",
    "# class BirdCLEFModel(nn.Module):\n",
    "#     def __init__(self, cfg, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "        \n",
    "#         self.backbone = timm.create_model(\n",
    "#             cfg.model_name,\n",
    "#             pretrained=False,  \n",
    "#             in_chans=cfg.in_channels,\n",
    "#             drop_rate=0.0,    \n",
    "#             drop_path_rate=0.0\n",
    "#         )\n",
    "        \n",
    "#         if 'efficientnet' in cfg.model_name:\n",
    "#             backbone_out = self.backbone.classifier.in_features\n",
    "#             self.backbone.classifier = nn.Identity()\n",
    "#         elif 'resnet' in cfg.model_name:\n",
    "#             backbone_out = self.backbone.fc.in_features\n",
    "#             self.backbone.fc = nn.Identity()\n",
    "#         else:\n",
    "#             backbone_out = self.backbone.get_classifier().in_features\n",
    "#             self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "#         self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.feat_dim = backbone_out\n",
    "#         self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         features = self.backbone(x)\n",
    "        \n",
    "#         if isinstance(features, dict):\n",
    "#             features = features['features']\n",
    "            \n",
    "#         if len(features.shape) == 4:\n",
    "#             features = self.pooling(features)\n",
    "#             features = features.view(features.size(0), -1)\n",
    "        \n",
    "#         logits = self.classifier(features)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "259244f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:16:19.063729Z",
     "iopub.status.busy": "2025-05-27T03:16:19.063349Z",
     "iopub.status.idle": "2025-05-27T03:16:19.069660Z",
     "shell.execute_reply": "2025-05-27T03:16:19.068640Z"
    },
    "papermill": {
     "duration": 0.01758,
     "end_time": "2025-05-27T03:16:19.071601",
     "exception": false,
     "start_time": "2025-05-27T03:16:19.054021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def audio2melspec(audio_data, cfg):\n",
    "#     \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "#     if np.isnan(audio_data).any():\n",
    "#         mean_signal = np.nanmean(audio_data)\n",
    "#         audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "#     mel_spec = librosa.feature.melspectrogram(\n",
    "#         y=audio_data,\n",
    "#         sr=cfg.FS,\n",
    "#         n_fft=cfg.N_FFT,\n",
    "#         hop_length=cfg.HOP_LENGTH,\n",
    "#         n_mels=cfg.N_MELS,\n",
    "#         fmin=cfg.FMIN,\n",
    "#         fmax=cfg.FMAX,\n",
    "#         power=2.0,\n",
    "#         pad_mode=\"reflect\",\n",
    "#         norm='slaney',\n",
    "#         htk=True,\n",
    "#         center=True,\n",
    "#     )\n",
    "\n",
    "#     mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "#     mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "#     return mel_spec_norm\n",
    "\n",
    "# def process_audio_segment(audio_data, cfg):\n",
    "#     \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "#     if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "#         audio_data = np.pad(audio_data, \n",
    "#                           (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "#                           mode='constant')\n",
    "    \n",
    "#     mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "#     # Resize if needed\n",
    "#     if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "#         mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "#     return mel_spec.astype(np.float32)\n",
    "    \n",
    "# def find_model_files(cfg):\n",
    "#     \"\"\"\n",
    "#     Find all .pth model files in the specified model directory\n",
    "#     \"\"\"\n",
    "#     model_files = []\n",
    "    \n",
    "#     model_dir = Path(cfg.model_path)\n",
    "    \n",
    "#     for path in model_dir.glob('**/*.pth'):\n",
    "#         model_files.append(str(path))\n",
    "    \n",
    "#     return model_files\n",
    "\n",
    "# def load_models(cfg, num_classes):\n",
    "#     \"\"\"\n",
    "#     Load all found model files and prepare them for ensemble\n",
    "#     \"\"\"\n",
    "#     models = []\n",
    "    \n",
    "#     model_files = find_model_files(cfg)\n",
    "    \n",
    "#     if not model_files:\n",
    "#         print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "#         return models\n",
    "    \n",
    "#     print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "#     if cfg.use_specific_folds:\n",
    "#         filtered_files = []\n",
    "#         for fold in cfg.folds:\n",
    "#             fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "#             filtered_files.extend(fold_files)\n",
    "#         model_files = filtered_files\n",
    "#         print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
    "    \n",
    "#     for model_path in model_files:\n",
    "#         try:\n",
    "#             print(f\"Loading model: {model_path}\")\n",
    "#             checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n",
    "            \n",
    "#             model = BirdCLEFModel(cfg, num_classes)\n",
    "#             model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#             model = model.to(cfg.device)\n",
    "#             model.eval()\n",
    "            \n",
    "#             models.append(model)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "#     return models\n",
    "\n",
    "# def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "#     \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "#     predictions = []\n",
    "#     row_ids = []\n",
    "#     soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"Processing {soundscape_id}\")\n",
    "#         audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "#         total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "#         for segment_idx in range(total_segments):\n",
    "#             start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "#             end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "#             segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "#             end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "#             row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "#             row_ids.append(row_id)\n",
    "\n",
    "#             if cfg.use_tta:\n",
    "#                 all_preds = []\n",
    "                \n",
    "#                 for tta_idx in range(cfg.tta_count):\n",
    "#                     mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "#                     mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "#                     mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "#                     mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "#                     if len(models) == 1:\n",
    "#                         with torch.no_grad():\n",
    "#                             outputs = models[0](mel_spec)\n",
    "#                             probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                             all_preds.append(probs)\n",
    "#                     else:\n",
    "#                         segment_preds = []\n",
    "#                         for model in models:\n",
    "#                             with torch.no_grad():\n",
    "#                                 outputs = model(mel_spec)\n",
    "#                                 probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                                 segment_preds.append(probs)\n",
    "                        \n",
    "#                         avg_preds = np.mean(segment_preds, axis=0)\n",
    "#                         all_preds.append(avg_preds)\n",
    "\n",
    "#                 final_preds = np.mean(all_preds, axis=0)\n",
    "#             else:\n",
    "#                 mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "#                 mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "#                 mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "#                 if len(models) == 1:\n",
    "#                     with torch.no_grad():\n",
    "#                         outputs = models[0](mel_spec)\n",
    "#                         final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                 else:\n",
    "#                     segment_preds = []\n",
    "#                     for model in models:\n",
    "#                         with torch.no_grad():\n",
    "#                             outputs = model(mel_spec)\n",
    "#                             probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                             segment_preds.append(probs)\n",
    "\n",
    "#                     final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "#             predictions.append(final_preds)\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {audio_path}: {e}\")\n",
    "    \n",
    "#     return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7292091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:16:19.091093Z",
     "iopub.status.busy": "2025-05-27T03:16:19.090710Z",
     "iopub.status.idle": "2025-05-27T03:16:19.095363Z",
     "shell.execute_reply": "2025-05-27T03:16:19.094449Z"
    },
    "papermill": {
     "duration": 0.017082,
     "end_time": "2025-05-27T03:16:19.097008",
     "exception": false,
     "start_time": "2025-05-27T03:16:19.079926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def apply_tta(spec, tta_idx):\n",
    "#     \"\"\"Apply test-time augmentation\"\"\"\n",
    "#     if tta_idx == 0:\n",
    "#         # Original spectrogram\n",
    "#         return spec\n",
    "#     elif tta_idx == 1:\n",
    "#         # Time shift (horizontal flip)\n",
    "#         return np.flip(spec, axis=1)\n",
    "#     elif tta_idx == 2:\n",
    "#         # Frequency shift (vertical flip)\n",
    "#         return np.flip(spec, axis=0)\n",
    "#     else:\n",
    "#         return spec\n",
    "\n",
    "# def run_inference(cfg, models, species_ids):\n",
    "#     \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "#     test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "#     if cfg.debug:\n",
    "#         print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "#         test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "#     print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "#     all_row_ids = []\n",
    "#     all_predictions = []\n",
    "\n",
    "#     for audio_path in tqdm(test_files):\n",
    "#         row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "#         all_row_ids.extend(row_ids)\n",
    "#         all_predictions.extend(predictions)\n",
    "    \n",
    "#     return all_row_ids, all_predictions\n",
    "\n",
    "# def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "#     \"\"\"Create submission dataframe\"\"\"\n",
    "#     print(\"Creating submission dataframe...\")\n",
    "\n",
    "#     submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "#     for i, species in enumerate(species_ids):\n",
    "#         submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "#     submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "#     submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "#     sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "#     missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "#     if missing_cols:\n",
    "#         print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "#         for col in missing_cols:\n",
    "#             submission_df[col] = 0.0\n",
    "\n",
    "#     submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "#     submission_df = submission_df.reset_index()\n",
    "    \n",
    "#     return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c224d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:16:19.114575Z",
     "iopub.status.busy": "2025-05-27T03:16:19.114200Z",
     "iopub.status.idle": "2025-05-27T03:16:19.118132Z",
     "shell.execute_reply": "2025-05-27T03:16:19.117277Z"
    },
    "papermill": {
     "duration": 0.014652,
     "end_time": "2025-05-27T03:16:19.119820",
     "exception": false,
     "start_time": "2025-05-27T03:16:19.105168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     start_time = time.time()\n",
    "#     print(\"Starting BirdCLEF-2025 inference...\")\n",
    "#     print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "#     models = load_models(cfg, num_classes)\n",
    "    \n",
    "#     if not models:\n",
    "#         print(\"No models found! Please check model paths.\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "#     row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "#     submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "#     submission_path = 'submission1.csv'\n",
    "#     submission_df.to_csv(submission_path, index=False)\n",
    "#     print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253a8256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:16:19.137625Z",
     "iopub.status.busy": "2025-05-27T03:16:19.137214Z",
     "iopub.status.idle": "2025-05-27T03:16:19.141392Z",
     "shell.execute_reply": "2025-05-27T03:16:19.140406Z"
    },
    "papermill": {
     "duration": 0.01528,
     "end_time": "2025-05-27T03:16:19.143298",
     "exception": false,
     "start_time": "2025-05-27T03:16:19.128018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('submission1.csv')\n",
    "# cols = sub.columns[1:]\n",
    "# groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "# groups = groups.values\n",
    "# for group in np.unique(groups):\n",
    "#     sub_group = sub[group == groups]\n",
    "#     predictions = sub_group[cols].values\n",
    "#     new_predictions = predictions.copy()\n",
    "#     for i in range(1, predictions.shape[0]-1):\n",
    "#         new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "#     new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "#     new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "#     sub_group[cols] = new_predictions\n",
    "#     sub[group == groups] = sub_group\n",
    "# sub.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2be31d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:16:19.161344Z",
     "iopub.status.busy": "2025-05-27T03:16:19.160994Z",
     "iopub.status.idle": "2025-05-27T03:17:07.249395Z",
     "shell.execute_reply": "2025-05-27T03:17:07.248215Z"
    },
    "papermill": {
     "duration": 48.099689,
     "end_time": "2025-05-27T03:17:07.251323",
     "exception": false,
     "start_time": "2025-05-27T03:16:19.151634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-2a17e6f6fea4>:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=cfg.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: test_soundscapes is empty. Using fallback train_soundscapes...\n",
      "Found 10 soundscapes for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission dataframe...\n",
      "✅ Saved submission1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "np.complex = complex  # 兼容 librosa 新版\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "class CFG:\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    FS = 32000\n",
    "    WINDOW_SIZE = 5\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (128, 128)\n",
    "    model_path = '/kaggle/input/bird25-d-330v2-ppv15-convnextv2-nano'\n",
    "    model_name = 'convnextv2_nano.fcmae_ft_in22k_in1k'\n",
    "    use_specific_folds = True\n",
    "    folds = [0]\n",
    "    in_channels = 1\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    batch_size = 16\n",
    "    use_tta = False\n",
    "    debug = False\n",
    "    debug_count = 2\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# ----------------- LOAD TAXONOMY -----------------\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "\n",
    "# ----------------- MODEL -----------------\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(cfg.model_name, pretrained=False, in_chans=cfg.in_channels)\n",
    "        out_dim = self.backbone.get_classifier().in_features\n",
    "        self.backbone.reset_classifier(0, '')\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(out_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features).view(features.size(0), -1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# ----------------- MEL SPECTROGRAM -----------------\n",
    "def audio2melspec(audio, cfg):\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX\n",
    "    )\n",
    "    db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    norm = (db - db.min()) / (db.max() - db.min() + 1e-6)\n",
    "    return norm\n",
    "\n",
    "def process_segment(audio_seg, cfg):\n",
    "    if len(audio_seg) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        pad_len = cfg.FS * cfg.WINDOW_SIZE - len(audio_seg)\n",
    "        audio_seg = np.pad(audio_seg, (0, pad_len), mode='constant')\n",
    "    mel = audio2melspec(audio_seg, cfg)\n",
    "    mel = cv2.resize(mel, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "    return mel.astype(np.float32)\n",
    "\n",
    "# ----------------- LOAD MODEL -----------------\n",
    "def load_models(cfg, num_classes):\n",
    "    model_files = list(Path(cfg.model_path).glob(\"**/*fold0*.pth\"))\n",
    "    models = []\n",
    "    for path in model_files:\n",
    "        model = BirdCLEFModel(cfg, num_classes)\n",
    "        checkpoint = torch.load(path, map_location=cfg.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(cfg.device).eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "# ----------------- INFERENCE ON SINGLE FILE -----------------\n",
    "def predict_on_audio(audio_path, models, cfg):\n",
    "    audio, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "    segments = len(audio) // (cfg.FS * cfg.WINDOW_SIZE)\n",
    "    predictions, row_ids = [], []\n",
    "    for i in range(segments):\n",
    "        seg = audio[i*cfg.FS*cfg.WINDOW_SIZE:(i+1)*cfg.FS*cfg.WINDOW_SIZE]\n",
    "        mel = process_segment(seg, cfg)\n",
    "        mel = torch.tensor(mel).unsqueeze(0).unsqueeze(0).to(cfg.device)\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(models[0](mel)).cpu().numpy().squeeze()\n",
    "        predictions.append(probs)\n",
    "        row_ids.append(f\"{Path(audio_path).stem}_{(i+1)*cfg.WINDOW_SIZE}\")\n",
    "    return row_ids, predictions\n",
    "\n",
    "# ----------------- RUN ALL FILES -----------------\n",
    "def run_all(cfg):\n",
    "    models = load_models(cfg, num_classes)\n",
    "\n",
    "    test_dir = Path(cfg.test_soundscapes)\n",
    "    files = list(test_dir.glob(\"*.ogg\"))\n",
    "\n",
    "    # ✅ Fallback when test set is hidden\n",
    "    if len(files) == 0:\n",
    "        print(\"⚠️ Warning: test_soundscapes is empty. Using fallback train_soundscapes...\")\n",
    "        fallback_dir = Path(\"/kaggle/input/birdclef-2025/train_soundscapes\")\n",
    "        files = sorted(fallback_dir.glob(\"*.ogg\"))[:10]\n",
    "\n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled: using only first {cfg.debug_count} files\")\n",
    "        files = files[:cfg.debug_count]\n",
    "\n",
    "    print(f\"Found {len(files)} soundscapes for inference.\")\n",
    "\n",
    "    all_rows, all_preds = [], []\n",
    "    for path in tqdm(files):\n",
    "        rows, preds = predict_on_audio(str(path), models, cfg)\n",
    "        all_rows.extend(rows)\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "    return all_rows, all_preds\n",
    "\n",
    "# ----------------- MAKE SUBMISSION -----------------\n",
    "def make_submission(row_ids, predictions, species_ids, cfg):\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "    submission_df = submission_df.reset_index()\n",
    "    submission_df.to_csv(\"submission1.csv\", index=False)\n",
    "    print(\"✅ Saved submission1.csv\")\n",
    "\n",
    "# ----------------- MAIN -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    row_ids, preds = run_all(cfg)\n",
    "    make_submission(row_ids, preds, species_ids, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0295908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:07.271559Z",
     "iopub.status.busy": "2025-05-27T03:17:07.271154Z",
     "iopub.status.idle": "2025-05-27T03:17:07.324618Z",
     "shell.execute_reply": "2025-05-27T03:17:07.323496Z"
    },
    "papermill": {
     "duration": 0.065613,
     "end_time": "2025-05-27T03:17:07.326504",
     "exception": false,
     "start_time": "2025-05-27T03:17:07.260891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H02_20230420_074000_5</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.012310</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H02_20230420_074000_10</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.039644</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H02_20230420_074000_15</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H02_20230420_074000_20</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H02_20230420_074000_25</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>H02_20230421_233500_40</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>0.010099</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.006459</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.004597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>H02_20230421_233500_45</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>H02_20230421_233500_50</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>H02_20230421_233500_55</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016743</td>\n",
       "      <td>0.079623</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.005373</td>\n",
       "      <td>0.019633</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.002030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>H02_20230421_233500_60</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019322</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.009443</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.002549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     row_id   1139490   1192948   1194042    126247   1346504  \\\n",
       "0     H02_20230420_074000_5  0.000246  0.000078  0.000879  0.000350  0.002873   \n",
       "1    H02_20230420_074000_10  0.000506  0.000108  0.001204  0.000234  0.000624   \n",
       "2    H02_20230420_074000_15  0.001512  0.000399  0.001817  0.001108  0.003780   \n",
       "3    H02_20230420_074000_20  0.001200  0.000567  0.002270  0.000827  0.003311   \n",
       "4    H02_20230420_074000_25  0.000919  0.000265  0.002320  0.000832  0.002375   \n",
       "..                      ...       ...       ...       ...       ...       ...   \n",
       "115  H02_20230421_233500_40  0.000748  0.001201  0.007340  0.010099  0.002844   \n",
       "116  H02_20230421_233500_45  0.000357  0.000130  0.000702  0.002114  0.001432   \n",
       "117  H02_20230421_233500_50  0.000089  0.000055  0.000656  0.000519  0.001924   \n",
       "118  H02_20230421_233500_55  0.000238  0.000143  0.000406  0.000343  0.002113   \n",
       "119  H02_20230421_233500_60  0.000153  0.000086  0.000648  0.000331  0.001443   \n",
       "\n",
       "       134933    135045   1462711   1462737  ...   yebfly1   yebsee1  \\\n",
       "0    0.002414  0.003691  0.000080  0.000145  ...  0.004721  0.016973   \n",
       "1    0.001533  0.039644  0.000149  0.000206  ...  0.003499  0.009494   \n",
       "2    0.002005  0.006978  0.000342  0.000762  ...  0.000737  0.008584   \n",
       "3    0.002815  0.010823  0.000255  0.001000  ...  0.002186  0.008547   \n",
       "4    0.004033  0.010989  0.000195  0.000440  ...  0.001410  0.014465   \n",
       "..        ...       ...       ...       ...  ...       ...       ...   \n",
       "115  0.000251  0.000845  0.000656  0.001768  ...  0.002713  0.002619   \n",
       "116  0.000424  0.000858  0.000156  0.000230  ...  0.000645  0.002303   \n",
       "117  0.000529  0.000543  0.000064  0.000068  ...  0.001581  0.007007   \n",
       "118  0.000233  0.000466  0.000139  0.000132  ...  0.016743  0.079623   \n",
       "119  0.000189  0.000621  0.000119  0.000123  ...  0.019322  0.024648   \n",
       "\n",
       "      yecspi2   yectyr1   yehbla2   yehcar1   yelori1   yeofly1   yercac1  \\\n",
       "0    0.000888  0.000161  0.000131  0.002759  0.000533  0.012310  0.000207   \n",
       "1    0.006294  0.000411  0.000172  0.002324  0.000666  0.006252  0.000376   \n",
       "2    0.001013  0.000137  0.000132  0.001145  0.000145  0.004924  0.000077   \n",
       "3    0.001696  0.000223  0.000171  0.002359  0.000391  0.006463  0.000164   \n",
       "4    0.001524  0.000207  0.000113  0.002605  0.000317  0.007409  0.000210   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "115  0.006459  0.000443  0.002938  0.012357  0.000090  0.003975  0.000897   \n",
       "116  0.000515  0.000205  0.000168  0.000870  0.000094  0.001475  0.000229   \n",
       "117  0.000259  0.000207  0.000093  0.001812  0.000120  0.004360  0.000875   \n",
       "118  0.000655  0.000599  0.005373  0.019633  0.000824  0.006150  0.000932   \n",
       "119  0.001269  0.000590  0.007062  0.009443  0.001013  0.005580  0.001241   \n",
       "\n",
       "       ywcpar  \n",
       "0    0.000734  \n",
       "1    0.000336  \n",
       "2    0.000382  \n",
       "3    0.001098  \n",
       "4    0.000931  \n",
       "..        ...  \n",
       "115  0.004597  \n",
       "116  0.000435  \n",
       "117  0.001302  \n",
       "118  0.002030  \n",
       "119  0.002549  \n",
       "\n",
       "[120 rows x 207 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceeb85d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:07.348017Z",
     "iopub.status.busy": "2025-05-27T03:17:07.347626Z",
     "iopub.status.idle": "2025-05-27T03:17:07.873620Z",
     "shell.execute_reply": "2025-05-27T03:17:07.872562Z"
    },
    "papermill": {
     "duration": 0.53894,
     "end_time": "2025-05-27T03:17:07.875722",
     "exception": false,
     "start_time": "2025-05-27T03:17:07.336782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287a94c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:07.897210Z",
     "iopub.status.busy": "2025-05-27T03:17:07.896775Z",
     "iopub.status.idle": "2025-05-27T03:17:07.902578Z",
     "shell.execute_reply": "2025-05-27T03:17:07.901451Z"
    },
    "papermill": {
     "duration": 0.018468,
     "end_time": "2025-05-27T03:17:07.904369",
     "exception": false,
     "start_time": "2025-05-27T03:17:07.885901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "\n",
    "    stage = 'train_bce'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_files = ['/kaggle/input/bird2025-sed-ckpt/sedmodel.pth'\n",
    "                  ]\n",
    " \n",
    "    model_name = 'seresnext26t_32x4d'  \n",
    "    pretrained = False\n",
    "    in_channels = 1\n",
    "\n",
    "    \n",
    "    SR = 32000\n",
    "    target_duration = 5\n",
    "    train_duration = 10\n",
    "    \n",
    "    \n",
    "    device = 'cpu'\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b60e5fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:07.925678Z",
     "iopub.status.busy": "2025-05-27T03:17:07.925273Z",
     "iopub.status.idle": "2025-05-27T03:17:07.934751Z",
     "shell.execute_reply": "2025-05-27T03:17:07.933572Z"
    },
    "papermill": {
     "duration": 0.02202,
     "end_time": "2025-05-27T03:17:07.936528",
     "exception": false,
     "start_time": "2025-05-27T03:17:07.914508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c560ad88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:07.958600Z",
     "iopub.status.busy": "2025-05-27T03:17:07.958189Z",
     "iopub.status.idle": "2025-05-27T03:17:07.971355Z",
     "shell.execute_reply": "2025-05-27T03:17:07.970212Z"
    },
    "papermill": {
     "duration": 0.026342,
     "end_time": "2025-05-27T03:17:07.973568",
     "exception": false,
     "start_time": "2025-05-27T03:17:07.947226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7265be99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:07.996202Z",
     "iopub.status.busy": "2025-05-27T03:17:07.995808Z",
     "iopub.status.idle": "2025-05-27T03:17:08.005994Z",
     "shell.execute_reply": "2025-05-27T03:17:08.004724Z"
    },
    "papermill": {
     "duration": 0.023209,
     "end_time": "2025-05-27T03:17:08.007906",
     "exception": false,
     "start_time": "2025-05-27T03:17:07.984697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14e39cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:08.029808Z",
     "iopub.status.busy": "2025-05-27T03:17:08.029382Z",
     "iopub.status.idle": "2025-05-27T03:17:08.053650Z",
     "shell.execute_reply": "2025-05-27T03:17:08.052531Z"
    },
    "papermill": {
     "duration": 0.037465,
     "end_time": "2025-05-27T03:17:08.055714",
     "exception": false,
     "start_time": "2025-05-27T03:17:08.018249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(cfg['n_mels'])\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg['model_name'],\n",
    "            pretrained=False,\n",
    "            in_chans=cfg['in_channels'],\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        if \"efficientnet\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if self.cfg['device'] == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_feature(self,x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # if self.training:\n",
    "        #    x = self.spec_augmenter(x)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "\n",
    "        audio = audio.float()\n",
    "        \n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "        if self.cfg['in_channels'] == 3:\n",
    "            spec = image_delta(spec)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "\n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        return torch.logit(clipwise_output)\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        x,_ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "        start = (\n",
    "            feat_time / 2 - feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train']) / 2\n",
    "        )\n",
    "        end = start + feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train'])\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        pred = self.attention_infer(start,end,x,time_att)\n",
    "\n",
    "        start_minus = max(0, start-tta_delta)\n",
    "        end_minus=end-tta_delta\n",
    "        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n",
    "\n",
    "        start_plus = start+tta_delta\n",
    "        end_plus=min(feat_time, end+tta_delta)\n",
    "        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n",
    "\n",
    "        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n",
    "        return pred\n",
    "        \n",
    "    def attention_infer(self,start,end,x,time_att):\n",
    "        feat = x[:, :, start:end]\n",
    "        # att = torch.softmax(time_att[:, :, start:end], dim=-1)\n",
    "        #             print(feat_time, start, end)\n",
    "        #             print(att_a.sum(), att.sum(), time_att.shape)\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        # clipwise_output = torch.sum(framewise_pred * att, dim=-1)\n",
    "        #logits = torch.sum(\n",
    "        #    self.att_block.cla(feat) * att,\n",
    "        #    dim=-1,\n",
    "        #)\n",
    "\n",
    "        # return clipwise_output\n",
    "        return framewise_pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fa0db2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:08.077461Z",
     "iopub.status.busy": "2025-05-27T03:17:08.077064Z",
     "iopub.status.idle": "2025-05-27T03:17:08.086031Z",
     "shell.execute_reply": "2025-05-27T03:17:08.084992Z"
    },
    "papermill": {
     "duration": 0.021905,
     "end_time": "2025-05-27T03:17:08.087956",
     "exception": false,
     "start_time": "2025-05-27T03:17:08.066051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sample(path, cfg):\n",
    "    audio, orig_sr = sf.read(path, dtype=\"float32\")\n",
    "    seconds = []\n",
    "    audio_length = cfg.SR * cfg.target_duration\n",
    "    step = audio_length\n",
    "    for i in range(audio_length, len(audio) + step, step):\n",
    "        start = max(0, i - audio_length)\n",
    "        end = start + audio_length\n",
    "        if end > len(audio):\n",
    "            pass\n",
    "        else:\n",
    "            seconds.append(int(end/cfg.SR))\n",
    "\n",
    "    audio = np.concatenate([audio,audio,audio])\n",
    "    audios = []\n",
    "    for i,second in enumerate(seconds):\n",
    "        end_seconds = int(second)\n",
    "        start_seconds = int(end_seconds - cfg.target_duration)\n",
    "    \n",
    "        end_index = int(cfg.SR * (end_seconds + (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        start_index = int(cfg.SR * (start_seconds - (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        end_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        start_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        y = audio[start_index:end_index].astype(np.float32)\n",
    "        if i==0:\n",
    "            y[:start_pad] = 0\n",
    "        elif i==(len(seconds)-1):\n",
    "            y[-end_pad:] = 0\n",
    "        audios.append(y)\n",
    "\n",
    "    return audios\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "693e7f29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:08.110146Z",
     "iopub.status.busy": "2025-05-27T03:17:08.109774Z",
     "iopub.status.idle": "2025-05-27T03:17:08.121693Z",
     "shell.execute_reply": "2025-05-27T03:17:08.120595Z"
    },
    "papermill": {
     "duration": 0.025337,
     "end_time": "2025-05-27T03:17:08.123566",
     "exception": false,
     "start_time": "2025-05-27T03:17:08.098229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # model_files = find_model_files(cfg)\n",
    "    model_files = cfg.model_files\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for i, model_path in enumerate(model_files):\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n",
    "            cfg_temp = checkpoint['cfg']\n",
    "            cfg_temp['device'] = cfg.device\n",
    "            \n",
    "            model = BirdCLEFModel(cfg_temp)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            model.zero_grad()\n",
    "            model.half().float()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    print(f\"Processing {soundscape_id}\")\n",
    "    audio_data = load_sample(audio_path, cfg)\n",
    "    for segment_idx, audio_input in enumerate(audio_data):\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.target_duration\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "        \n",
    "        mel_spec = torch.tensor(audio_input, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        mel_spec = mel_spec.to(cfg.device)\n",
    "        \n",
    "        if len(models) == 1:\n",
    "            with torch.no_grad():\n",
    "                outputs = models[0].infer(mel_spec)\n",
    "                final_preds = outputs.squeeze()\n",
    "                # final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "\n",
    "        else:\n",
    "            segment_preds = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.infer(mel_spec)\n",
    "                    probs = outputs.squeeze()\n",
    "                    # probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                    segment_preds.append(probs)\n",
    "\n",
    "            \n",
    "            final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "        predictions.append(final_preds)\n",
    "\n",
    "    predictions = np.stack(predictions,axis=0)\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "980b2d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:08.145717Z",
     "iopub.status.busy": "2025-05-27T03:17:08.145280Z",
     "iopub.status.idle": "2025-05-27T03:17:08.158564Z",
     "shell.execute_reply": "2025-05-27T03:17:08.157373Z"
    },
    "papermill": {
     "duration": 0.026701,
     "end_time": "2025-05-27T03:17:08.160764",
     "exception": false,
     "start_time": "2025-05-27T03:17:08.134063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:10]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(\n",
    "        executor.map(\n",
    "            predict_on_spectrogram,\n",
    "            test_files,\n",
    "            itertools.repeat(models),\n",
    "            itertools.repeat(cfg),\n",
    "            itertools.repeat(species_ids)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def smooth_submission(submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n",
    "        \n",
    "        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n",
    "        are averaged with those of its neighbors using defined weights.\n",
    "        \n",
    "        :param submission_path: Path to the submission CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        # Extract group names by splitting row_id on the last underscore\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Smooth the predictions using neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "            # Replace the smoothed values in the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd69e25d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:08.183019Z",
     "iopub.status.busy": "2025-05-27T03:17:08.182602Z",
     "iopub.status.idle": "2025-05-27T03:17:08.189027Z",
     "shell.execute_reply": "2025-05-27T03:17:08.187770Z"
    },
    "papermill": {
     "duration": 0.019917,
     "end_time": "2025-05-27T03:17:08.191356",
     "exception": false,
     "start_time": "2025-05-27T03:17:08.171439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission2.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    smooth_submission(submission_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe2038fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:08.213636Z",
     "iopub.status.busy": "2025-05-27T03:17:08.213224Z",
     "iopub.status.idle": "2025-05-27T03:17:49.010283Z",
     "shell.execute_reply": "2025-05-27T03:17:49.008705Z"
    },
    "papermill": {
     "duration": 40.810132,
     "end_time": "2025-05-27T03:17:49.012143",
     "exception": false,
     "start_time": "2025-05-27T03:17:08.202011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "Found a total of 1 model files.\n",
      "Loading model: /kaggle/input/bird2025-sed-ckpt/sedmodel.pth\n",
      "Model usage: Single model\n",
      "Found 10 test soundscapes\n",
      "Processing H02_20230420_074000\n",
      "Processing H02_20230420_112000\n",
      "Processing H02_20230420_154500\n",
      "Processing H02_20230420_164000\n",
      "Processing H02_20230420_223500\n",
      "Processing H02_20230421_093000\n",
      "Processing H02_20230421_113500\n",
      "Processing H02_20230421_170000\n",
      "Processing H02_20230421_190500\n",
      "Processing H02_20230421_233500\n",
      "Creating submission dataframe...\n",
      "Submission saved to submission2.csv\n",
      "Smoothing submission predictions...\n",
      "Smoothed submission saved to submission2.csv\n",
      "Inference completed in 0.68 minutes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29137f58",
   "metadata": {
    "papermill": {
     "duration": 0.010643,
     "end_time": "2025-05-27T03:17:49.033835",
     "exception": false,
     "start_time": "2025-05-27T03:17:49.023192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Submission2(nfnet)》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3031c1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:49.057105Z",
     "iopub.status.busy": "2025-05-27T03:17:49.056728Z",
     "iopub.status.idle": "2025-05-27T03:17:49.063425Z",
     "shell.execute_reply": "2025-05-27T03:17:49.062237Z"
    },
    "papermill": {
     "duration": 0.020528,
     "end_time": "2025-05-27T03:17:49.065180",
     "exception": false,
     "start_time": "2025-05-27T03:17:49.044652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "def apply_power_to_low_ranked_cols(\n",
    "    p: np.ndarray,\n",
    "    top_k: int = 30,\n",
    "    exponent: Union[int, float] = 2,\n",
    "    inplace: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rank columns by their column‑wise maximum and raise every column whose\n",
    "    rank falls below `top_k` to a given power.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "        A 2‑D array of shape **(n_chunks, n_classes)**.\n",
    "\n",
    "        - **n_chunks** is the number of fixed‑length time chunks obtained\n",
    "          after slicing the input audio (or other sequential data).  \n",
    "          *Example:* In the BirdCLEF `test_soundscapes` set, each file is\n",
    "          60 s long. If you extract non‑overlapping 5 s windows,  \n",
    "          `n_chunks = 60 s / 5 s = 12`.\n",
    "        - **n_classes** is the number of classes being predicted.\n",
    "        - Each element `p[i, j]` is the score or probability of class *j*\n",
    "          in chunk *i*.\n",
    "\n",
    "    top_k : int, default=30\n",
    "        The highest‑ranked columns (by their maximum value) that remain\n",
    "        unchanged.\n",
    "\n",
    "    exponent : int or float, default=2\n",
    "        The power applied to the selected low‑ranked columns  \n",
    "        (e.g. `2` squares, `0.5` takes the square root, `3` cubes).\n",
    "\n",
    "    inplace : bool, default=True\n",
    "        If `True`, modify `p` in place.  \n",
    "        If `False`, operate on a copy and leave the original array intact.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The transformed array. It is the same object as `p` when\n",
    "        `inplace=True`; otherwise, it is a new array.\n",
    "\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        p = p.copy()\n",
    "\n",
    "    # Identify columns whose max value ranks below `top_k`\n",
    "    tail_cols = np.argsort(-p.max(axis=0))[top_k:]\n",
    "\n",
    "    # Apply the power transformation to those columns\n",
    "    p[:, tail_cols] = p[:, tail_cols] ** exponent\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf0d85dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:49.088608Z",
     "iopub.status.busy": "2025-05-27T03:17:49.088197Z",
     "iopub.status.idle": "2025-05-27T03:17:49.093669Z",
     "shell.execute_reply": "2025-05-27T03:17:49.092536Z"
    },
    "papermill": {
     "duration": 0.019434,
     "end_time": "2025-05-27T03:17:49.095732",
     "exception": false,
     "start_time": "2025-05-27T03:17:49.076298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from contextlib import contextmanager\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57ca7e67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:49.119897Z",
     "iopub.status.busy": "2025-05-27T03:17:49.119542Z",
     "iopub.status.idle": "2025-05-27T03:17:49.125945Z",
     "shell.execute_reply": "2025-05-27T03:17:49.124826Z"
    },
    "papermill": {
     "duration": 0.020811,
     "end_time": "2025-05-27T03:17:49.127745",
     "exception": false,
     "start_time": "2025-05-27T03:17:49.106934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: False\n",
      "Number of test soundscapes: 0\n"
     ]
    }
   ],
   "source": [
    "test_audio_dir = '../input/birdclef-2025/test_soundscapes/'\n",
    "file_list = [f for f in sorted(os.listdir(test_audio_dir))]\n",
    "file_list = [file.split('.')[0] for file in file_list if file.endswith('.ogg')]\n",
    "\n",
    "debug = False\n",
    "print('Debug mode:', debug)\n",
    "print('Number of test soundscapes:', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "428e4700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:49.151332Z",
     "iopub.status.busy": "2025-05-27T03:17:49.150987Z",
     "iopub.status.idle": "2025-05-27T03:17:49.167525Z",
     "shell.execute_reply": "2025-05-27T03:17:49.166282Z"
    },
    "papermill": {
     "duration": 0.030826,
     "end_time": "2025-05-27T03:17:49.169719",
     "exception": false,
     "start_time": "2025-05-27T03:17:49.138893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wav_sec = 5\n",
    "sample_rate = 32000\n",
    "min_segment = sample_rate*wav_sec\n",
    "\n",
    "class_labels = sorted(os.listdir('../input/birdclef-2025/train_audio/'))\n",
    "\n",
    "n_fft=1024\n",
    "win_length=1024\n",
    "hop_length=512\n",
    "f_min=50\n",
    "f_max=16000\n",
    "n_mels=128\n",
    "\n",
    "mel_spectrogram = AT.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    f_min=f_min,\n",
    "    f_max=f_max,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    "    # normalized=True\n",
    ")\n",
    "\n",
    "def normalize_std(spec, eps=1e-6):\n",
    "    mean = torch.mean(spec)\n",
    "    std = torch.std(spec)\n",
    "    return torch.where(std == 0, spec-mean, (spec - mean) / (std+eps))\n",
    "\n",
    "def audio_to_mel(filepath=None):\n",
    "    waveform, sample_rate = torchaudio.load(filepath,backend=\"soundfile\")\n",
    "    len_wav = waveform.shape[1]\n",
    "    waveform = waveform[0,:].reshape(1, len_wav) # stereo->mono mono->mono\n",
    "    PREDS = []\n",
    "    for i in range(12):\n",
    "        waveform2 = waveform[:,i*sample_rate*5:i*sample_rate*5+sample_rate*5]\n",
    "        melspec = mel_spectrogram(waveform2)\n",
    "        melspec = torch.log(melspec+1e-6)\n",
    "        melspec = normalize_std(melspec)\n",
    "        melspec = torch.unsqueeze(melspec, dim=0)\n",
    "        \n",
    "        PREDS.append(melspec)\n",
    "    return torch.vstack(PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "321f5cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:49.194278Z",
     "iopub.status.busy": "2025-05-27T03:17:49.193877Z",
     "iopub.status.idle": "2025-05-27T03:17:49.213035Z",
     "shell.execute_reply": "2025-05-27T03:17:49.211825Z"
    },
    "papermill": {
     "duration": 0.034039,
     "end_time": "2025-05-27T03:17:49.215255",
     "exception": false,
     "start_time": "2025-05-27T03:17:49.181216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1, n_mels=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block2 = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data.transpose(2,3)\n",
    "        x = torch.cat((x,x,x),1)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block2(x)\n",
    "        logit = torch.sum(norm_att * self.att_block2.cla(x), dim=2)\n",
    "\n",
    "        output_dict = {\n",
    "            'logit': logit,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fdd3c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:49.239011Z",
     "iopub.status.busy": "2025-05-27T03:17:49.238641Z",
     "iopub.status.idle": "2025-05-27T03:17:49.245558Z",
     "shell.execute_reply": "2025-05-27T03:17:49.244282Z"
    },
    "papermill": {
     "duration": 0.020823,
     "end_time": "2025-05-27T03:17:49.247392",
     "exception": false,
     "start_time": "2025-05-27T03:17:49.226569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/birdclef-2025-sed-models-p/sed0.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed1.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed2.pth']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name='eca_nfnet_l0'\n",
    "pretrained=False\n",
    "in_channels=3\n",
    "\n",
    "MODELS = [f'/kaggle/input/birdclef-2025-sed-models-p/sed{i}.pth' for i in range(3)]\n",
    "\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24f23571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:49.271562Z",
     "iopub.status.busy": "2025-05-27T03:17:49.271085Z",
     "iopub.status.idle": "2025-05-27T03:17:54.031163Z",
     "shell.execute_reply": "2025-05-27T03:17:54.030176Z"
    },
    "papermill": {
     "duration": 4.774527,
     "end_time": "2025-05-27T03:17:54.033197",
     "exception": false,
     "start_time": "2025-05-27T03:17:49.258670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for path in MODELS:\n",
    "    model = TimmSED(base_model_name=base_model_name,\n",
    "               pretrained=pretrained,\n",
    "               num_classes=len(class_labels),\n",
    "               in_channels=in_channels,\n",
    "               n_mels=n_mels);\n",
    "    model.load_state_dict(torch.load(path, weights_only=True, map_location=torch.device('cpu')))\n",
    "    model.eval();\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "516bfb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:54.060791Z",
     "iopub.status.busy": "2025-05-27T03:17:54.060301Z",
     "iopub.status.idle": "2025-05-27T03:17:54.067719Z",
     "shell.execute_reply": "2025-05-27T03:17:54.066551Z"
    },
    "papermill": {
     "duration": 0.023349,
     "end_time": "2025-05-27T03:17:54.069717",
     "exception": false,
     "start_time": "2025-05-27T03:17:54.046368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(afile):    \n",
    "    global pred\n",
    "    path = test_audio_dir + afile + '.ogg'\n",
    "    with torch.inference_mode():\n",
    "        sig = audio_to_mel(path)\n",
    "        outputs = None\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            p = model(sig)\n",
    "            p = torch.sigmoid(p['logit']).detach().cpu().numpy() \n",
    "            p = apply_power_to_low_ranked_cols(p, top_k=30,exponent=2)\n",
    "            if outputs is None: outputs = p\n",
    "            else: outputs += p\n",
    "            \n",
    "        outputs /= len(models)\n",
    "        chunks = [[] for i in range(12)]\n",
    "        for i in range(len(chunks)):        \n",
    "            chunk_end_time = (i + 1) * 5\n",
    "            row_id = afile + '_' + str(chunk_end_time)\n",
    "            pred['row_id'].append(row_id)\n",
    "            bird_no = 0\n",
    "            for bird in class_labels:         \n",
    "                pred[bird].append(outputs[i,bird_no])\n",
    "                bird_no += 1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76d66100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:54.093867Z",
     "iopub.status.busy": "2025-05-27T03:17:54.093517Z",
     "iopub.status.idle": "2025-05-27T03:17:54.099683Z",
     "shell.execute_reply": "2025-05-27T03:17:54.098591Z"
    },
    "papermill": {
     "duration": 0.020372,
     "end_time": "2025-05-27T03:17:54.101633",
     "exception": false,
     "start_time": "2025-05-27T03:17:54.081261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = {'row_id': []}\n",
    "for species_code in class_labels:\n",
    "    pred[species_code] = []\n",
    "    \n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    _ = list(executor.map(prediction, file_list))\n",
    "end_t = time.time()\n",
    "\n",
    "if debug == True:\n",
    "    print(700*(end_t - start)/60/debug_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "818668b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:54.126539Z",
     "iopub.status.busy": "2025-05-27T03:17:54.126048Z",
     "iopub.status.idle": "2025-05-27T03:17:54.143284Z",
     "shell.execute_reply": "2025-05-27T03:17:54.142172Z"
    },
    "papermill": {
     "duration": 0.032021,
     "end_time": "2025-05-27T03:17:54.145154",
     "exception": false,
     "start_time": "2025-05-27T03:17:54.113133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, 1139490, 1192948, 1194042, 126247, 1346504, 134933, 135045, 1462711, 1462737, 1564122, 21038, 21116, 21211, 22333, 22973, 22976, 24272, 24292, 24322, 41663, 41778, 41970, 42007, 42087, 42113, 46010, 47067, 476537, 476538, 48124, 50186, 517119, 523060, 528041, 52884, 548639, 555086, 555142, 566513, 64862, 65336, 65344, 65349, 65373, 65419, 65448, 65547, 65962, 66016, 66531, 66578, 66893, 67082, 67252, 714022, 715170, 787625, 81930, 868458, 963335, amakin1, amekes, ampkin1, anhing, babwar, bafibi1, banana, baymac, bbwduc, bicwre1, bkcdon, bkmtou1, blbgra1, blbwre1, blcant4, blchaw1, blcjay1, blctit1, blhpar1, blkvul, bobfly1, bobher1, brtpar1, bubcur1, bubwre1, bucmot3, bugtan, butsal1, cargra1, cattyr, chbant1, chfmac1, cinbec1, cocher1, cocwoo1, colara1, colcha1, compau, compot1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 207 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(pred, columns = ['row_id'] + class_labels) \n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8f2bfa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:54.170204Z",
     "iopub.status.busy": "2025-05-27T03:17:54.169834Z",
     "iopub.status.idle": "2025-05-27T03:17:54.202219Z",
     "shell.execute_reply": "2025-05-27T03:17:54.201159Z"
    },
    "papermill": {
     "duration": 0.047278,
     "end_time": "2025-05-27T03:17:54.204257",
     "exception": false,
     "start_time": "2025-05-27T03:17:54.156979",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"submission3.csv\", index=False)    \n",
    "\n",
    "sub = pd.read_csv('submission3.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "\n",
    "if debug:\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d456261",
   "metadata": {
    "papermill": {
     "duration": 0.011249,
     "end_time": "2025-05-27T03:17:54.227230",
     "exception": false,
     "start_time": "2025-05-27T03:17:54.215981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Finaly Blending》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbe5b63e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:54.252025Z",
     "iopub.status.busy": "2025-05-27T03:17:54.251655Z",
     "iopub.status.idle": "2025-05-27T03:17:54.256307Z",
     "shell.execute_reply": "2025-05-27T03:17:54.255086Z"
    },
    "papermill": {
     "duration": 0.019589,
     "end_time": "2025-05-27T03:17:54.258434",
     "exception": false,
     "start_time": "2025-05-27T03:17:54.238845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------- #\n",
    "# [IMPORTANT]\n",
    "# * Blending Weight\n",
    "# ------------------------------------------ #\n",
    "sub_w=[0.15,0.3, 0.55] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94dd5057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:17:54.284345Z",
     "iopub.status.busy": "2025-05-27T03:17:54.283980Z",
     "iopub.status.idle": "2025-05-27T03:17:58.125110Z",
     "shell.execute_reply": "2025-05-27T03:17:58.123750Z"
    },
    "papermill": {
     "duration": 3.85576,
     "end_time": "2025-05-27T03:17:58.127009",
     "exception": false,
     "start_time": "2025-05-27T03:17:54.271249",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Blended submission saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 权重设置\n",
    "sub_w = [0.15, 0.65, 0.20]  # 三个模型对应的权重，必须和下面的 submission 文件一一对应\n",
    "submission_paths = [\n",
    "    \"/kaggle/working/submission1.csv\",\n",
    "    \"/kaggle/working/submission2.csv\",\n",
    "    \"/kaggle/working/submission3.csv\"\n",
    "]\n",
    "\n",
    "# 加载所有 submission\n",
    "dfs = [pd.read_csv(path) for path in submission_paths]\n",
    "species_cols = [col for col in dfs[0].columns if col != 'row_id']\n",
    "\n",
    "# 重命名列：每列加编号后缀\n",
    "for i, df in enumerate(dfs):\n",
    "    dfs[i] = df.rename(columns={col: f\"{col} {i}\" for col in species_cols})\n",
    "\n",
    "# 依次 merge\n",
    "merged_df = dfs[0]\n",
    "for i in range(1, len(dfs)):\n",
    "    merged_df = pd.merge(merged_df, dfs[i], on=\"row_id\")\n",
    "\n",
    "# 融合预测\n",
    "for col in species_cols:\n",
    "    merged_df[col] = sum(merged_df[f\"{col} {i}\"] * sub_w[i] for i in range(len(dfs)))\n",
    "\n",
    "# 删除临时列\n",
    "for col in species_cols:\n",
    "    for i in range(len(dfs)):\n",
    "        del merged_df[f\"{col} {i}\"]\n",
    "\n",
    "# 保存最终提交文件\n",
    "merged_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ Blended submission saved as submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7430593,
     "sourceId": 11828260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7457365,
     "sourceId": 11867185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 106.215941,
   "end_time": "2025-05-27T03:18:01.523266",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-27T03:16:15.307325",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
