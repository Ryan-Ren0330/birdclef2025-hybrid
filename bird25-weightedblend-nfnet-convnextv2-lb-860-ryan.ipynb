{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b07b61d",
   "metadata": {
    "papermill": {
     "duration": 0.006488,
     "end_time": "2025-05-27T03:34:24.372967",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.366479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Only Submission(LoadLocalTrainModel)\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c10e7",
   "metadata": {
    "papermill": {
     "duration": 0.005332,
     "end_time": "2025-05-27T03:34:24.383976",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.378644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **ℹ️INFO**\n",
    "* This notebook is an weighted blend(nfnet * 0.6 + convnextv2 * 0.4).\n",
    "    * **GreatWork LB.850(nfnet)** https://www.kaggle.com/code/myso1987/post-processing-with-power-adjustment-for-low-rank\n",
    "    * **MyLocalTrainModel(convnextv2)** https://www.kaggle.com/datasets/hideyukizushi/bird25-d-330v2-ppv15-convnextv2-nano/data\n",
    "\n",
    "### **ℹ️WeightedBlend**\n",
    "* In CV tasks, it is common to adopt diverse backbones to ensure robustness, and I am sharing this in this competition because it is connected to the LB boost.\n",
    "* P.S. Adjusting the blending weights should make it fit LB better and improve the score. In that case, it is redundant, so I recommend submitting in a private notebook rather than a public notebook.\n",
    "\n",
    "### **ℹ️Appendix**\n",
    "* My old LB.829 notebook from this competition\n",
    "* https://www.kaggle.com/code/hideyukizushi/bird25-onlyinf-v2-s-focallossbce-cv-962-lb-829\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a928cbd",
   "metadata": {
    "papermill": {
     "duration": 0.005379,
     "end_time": "2025-05-27T03:34:24.395081",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.389702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Submission1(convnextv2)》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4635cf8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:24.406843Z",
     "iopub.status.busy": "2025-05-27T03:34:24.406526Z",
     "iopub.status.idle": "2025-05-27T03:34:24.411118Z",
     "shell.execute_reply": "2025-05-27T03:34:24.410442Z"
    },
    "papermill": {
     "duration": 0.011823,
     "end_time": "2025-05-27T03:34:24.412207",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.400384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "# import warnings\n",
    "# import logging\n",
    "# import time\n",
    "# import math\n",
    "# import cv2\n",
    "# from pathlib import Path\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import librosa\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import timm\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# logging.basicConfig(level=logging.ERROR)\n",
    "# print(\"Finished import group000000000 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7c3475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:24.424652Z",
     "iopub.status.busy": "2025-05-27T03:34:24.424415Z",
     "iopub.status.idle": "2025-05-27T03:34:24.427765Z",
     "shell.execute_reply": "2025-05-27T03:34:24.427001Z"
    },
    "papermill": {
     "duration": 0.010484,
     "end_time": "2025-05-27T03:34:24.428904",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.418420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CFG:\n",
    " \n",
    "#     test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "#     submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "#     taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    \n",
    "#     # ------------------------------------------- #\n",
    "#     # [IMPORTANT]\n",
    "#     # * Melspectrogram & Audio Params\n",
    "#     # ------------------------------------------- #\n",
    "#     FS = 32000  \n",
    "#     WINDOW_SIZE = 5\n",
    "#     N_FFT = 2048\n",
    "#     HOP_LENGTH = 512\n",
    "#     N_MELS = 512\n",
    "#     FMIN = 20\n",
    "#     FMAX = 16000\n",
    "#     TARGET_SHAPE = (256, 256)\n",
    "\n",
    "#     # ------------------------------------------- #\n",
    "#     # * Model def\n",
    "#     # ------------------------------------------- #\n",
    "#     model_path = '/kaggle/input/bird25-d-330v2-ppv15-convnextv2-nano'\n",
    "#     model_name = 'convnextv2_nano.fcmae_ft_in22k_in1k'\n",
    "#     use_specific_folds = True\n",
    "#     folds = [0,1]\n",
    "#     in_channels = 1\n",
    "#     device = 'cpu'  \n",
    "    \n",
    "#     # Inference parameters\n",
    "#     batch_size = 16\n",
    "#     use_tta = False  \n",
    "#     tta_count = 3\n",
    "#     threshold = 0.5\n",
    "\n",
    "#     # util\n",
    "#     debug = False\n",
    "#     debug_count = 3\n",
    "\n",
    "# cfg = CFG()\n",
    "\n",
    "# print(f\"Using device: {cfg.device}\")\n",
    "# print(f\"Loading taxonomy data...\")\n",
    "# taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "# species_ids = taxonomy_df['primary_label'].tolist()\n",
    "# num_classes = len(species_ids)\n",
    "# print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a91df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:24.440609Z",
     "iopub.status.busy": "2025-05-27T03:34:24.440403Z",
     "iopub.status.idle": "2025-05-27T03:34:24.444164Z",
     "shell.execute_reply": "2025-05-27T03:34:24.443313Z"
    },
    "papermill": {
     "duration": 0.011156,
     "end_time": "2025-05-27T03:34:24.445591",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.434435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class GeM(nn.Module):\n",
    "#     def __init__(self, p=3, eps=1e-6):\n",
    "#         super(GeM, self).__init__()\n",
    "#         self.p = nn.Parameter(torch.ones(1)*p)\n",
    "#         self.eps = eps\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "#     def gem(self, x, p=3, eps=1e-6):\n",
    "#         return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + \\\n",
    "#                 '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "#                 ', ' + 'eps=' + str(self.eps) + ')'\n",
    "# class BirdCLEFModel(nn.Module):\n",
    "#     def __init__(self, cfg, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "        \n",
    "#         self.backbone = timm.create_model(\n",
    "#             cfg.model_name,\n",
    "#             pretrained=False,  \n",
    "#             in_chans=cfg.in_channels,\n",
    "#             drop_rate=0.0,    \n",
    "#             drop_path_rate=0.0\n",
    "#         )\n",
    "        \n",
    "#         if 'efficientnet' in cfg.model_name:\n",
    "#             backbone_out = self.backbone.classifier.in_features\n",
    "#             self.backbone.classifier = nn.Identity()\n",
    "#         elif 'resnet' in cfg.model_name:\n",
    "#             backbone_out = self.backbone.fc.in_features\n",
    "#             self.backbone.fc = nn.Identity()\n",
    "#         else:\n",
    "#             backbone_out = self.backbone.get_classifier().in_features\n",
    "#             self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "#         self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.feat_dim = backbone_out\n",
    "#         self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         features = self.backbone(x)\n",
    "        \n",
    "#         if isinstance(features, dict):\n",
    "#             features = features['features']\n",
    "            \n",
    "#         if len(features.shape) == 4:\n",
    "#             features = self.pooling(features)\n",
    "#             features = features.view(features.size(0), -1)\n",
    "        \n",
    "#         logits = self.classifier(features)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91183a75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:24.458738Z",
     "iopub.status.busy": "2025-05-27T03:34:24.458442Z",
     "iopub.status.idle": "2025-05-27T03:34:24.462816Z",
     "shell.execute_reply": "2025-05-27T03:34:24.462118Z"
    },
    "papermill": {
     "duration": 0.01201,
     "end_time": "2025-05-27T03:34:24.464041",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.452031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def audio2melspec(audio_data, cfg):\n",
    "#     \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "#     if np.isnan(audio_data).any():\n",
    "#         mean_signal = np.nanmean(audio_data)\n",
    "#         audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "#     mel_spec = librosa.feature.melspectrogram(\n",
    "#         y=audio_data,\n",
    "#         sr=cfg.FS,\n",
    "#         n_fft=cfg.N_FFT,\n",
    "#         hop_length=cfg.HOP_LENGTH,\n",
    "#         n_mels=cfg.N_MELS,\n",
    "#         fmin=cfg.FMIN,\n",
    "#         fmax=cfg.FMAX,\n",
    "#         power=2.0,\n",
    "#         pad_mode=\"reflect\",\n",
    "#         norm='slaney',\n",
    "#         htk=True,\n",
    "#         center=True,\n",
    "#     )\n",
    "\n",
    "#     mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "#     mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "#     return mel_spec_norm\n",
    "\n",
    "# def process_audio_segment(audio_data, cfg):\n",
    "#     \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "#     if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "#         audio_data = np.pad(audio_data, \n",
    "#                           (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "#                           mode='constant')\n",
    "    \n",
    "#     mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "#     # Resize if needed\n",
    "#     if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "#         mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "#     return mel_spec.astype(np.float32)\n",
    "    \n",
    "# def find_model_files(cfg):\n",
    "#     \"\"\"\n",
    "#     Find all .pth model files in the specified model directory\n",
    "#     \"\"\"\n",
    "#     model_files = []\n",
    "    \n",
    "#     model_dir = Path(cfg.model_path)\n",
    "    \n",
    "#     for path in model_dir.glob('**/*.pth'):\n",
    "#         model_files.append(str(path))\n",
    "    \n",
    "#     return model_files\n",
    "\n",
    "# def load_models(cfg, num_classes):\n",
    "#     \"\"\"\n",
    "#     Load all found model files and prepare them for ensemble\n",
    "#     \"\"\"\n",
    "#     models = []\n",
    "    \n",
    "#     model_files = find_model_files(cfg)\n",
    "    \n",
    "#     if not model_files:\n",
    "#         print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "#         return models\n",
    "    \n",
    "#     print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "#     if cfg.use_specific_folds:\n",
    "#         filtered_files = []\n",
    "#         for fold in cfg.folds:\n",
    "#             fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "#             filtered_files.extend(fold_files)\n",
    "#         model_files = filtered_files\n",
    "#         print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
    "    \n",
    "#     for model_path in model_files:\n",
    "#         try:\n",
    "#             print(f\"Loading model: {model_path}\")\n",
    "#             checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n",
    "            \n",
    "#             model = BirdCLEFModel(cfg, num_classes)\n",
    "#             model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#             model = model.to(cfg.device)\n",
    "#             model.eval()\n",
    "            \n",
    "#             models.append(model)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "#     return models\n",
    "\n",
    "# def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "#     \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "#     predictions = []\n",
    "#     row_ids = []\n",
    "#     soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"Processing {soundscape_id}\")\n",
    "#         audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "#         total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "#         for segment_idx in range(total_segments):\n",
    "#             start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "#             end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "#             segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "#             end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "#             row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "#             row_ids.append(row_id)\n",
    "\n",
    "#             if cfg.use_tta:\n",
    "#                 all_preds = []\n",
    "                \n",
    "#                 for tta_idx in range(cfg.tta_count):\n",
    "#                     mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "#                     mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "#                     mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "#                     mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "#                     if len(models) == 1:\n",
    "#                         with torch.no_grad():\n",
    "#                             outputs = models[0](mel_spec)\n",
    "#                             probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                             all_preds.append(probs)\n",
    "#                     else:\n",
    "#                         segment_preds = []\n",
    "#                         for model in models:\n",
    "#                             with torch.no_grad():\n",
    "#                                 outputs = model(mel_spec)\n",
    "#                                 probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                                 segment_preds.append(probs)\n",
    "                        \n",
    "#                         avg_preds = np.mean(segment_preds, axis=0)\n",
    "#                         all_preds.append(avg_preds)\n",
    "\n",
    "#                 final_preds = np.mean(all_preds, axis=0)\n",
    "#             else:\n",
    "#                 mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "#                 mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "#                 mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "#                 if len(models) == 1:\n",
    "#                     with torch.no_grad():\n",
    "#                         outputs = models[0](mel_spec)\n",
    "#                         final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                 else:\n",
    "#                     segment_preds = []\n",
    "#                     for model in models:\n",
    "#                         with torch.no_grad():\n",
    "#                             outputs = model(mel_spec)\n",
    "#                             probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                             segment_preds.append(probs)\n",
    "\n",
    "#                     final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "#             predictions.append(final_preds)\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {audio_path}: {e}\")\n",
    "    \n",
    "#     return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2802e85c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:24.477435Z",
     "iopub.status.busy": "2025-05-27T03:34:24.477167Z",
     "iopub.status.idle": "2025-05-27T03:34:24.480465Z",
     "shell.execute_reply": "2025-05-27T03:34:24.479812Z"
    },
    "papermill": {
     "duration": 0.011319,
     "end_time": "2025-05-27T03:34:24.481604",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.470285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def apply_tta(spec, tta_idx):\n",
    "#     \"\"\"Apply test-time augmentation\"\"\"\n",
    "#     if tta_idx == 0:\n",
    "#         # Original spectrogram\n",
    "#         return spec\n",
    "#     elif tta_idx == 1:\n",
    "#         # Time shift (horizontal flip)\n",
    "#         return np.flip(spec, axis=1)\n",
    "#     elif tta_idx == 2:\n",
    "#         # Frequency shift (vertical flip)\n",
    "#         return np.flip(spec, axis=0)\n",
    "#     else:\n",
    "#         return spec\n",
    "\n",
    "# def run_inference(cfg, models, species_ids):\n",
    "#     \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "#     test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "#     if cfg.debug:\n",
    "#         print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "#         test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "#     print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "#     all_row_ids = []\n",
    "#     all_predictions = []\n",
    "\n",
    "#     for audio_path in tqdm(test_files):\n",
    "#         row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "#         all_row_ids.extend(row_ids)\n",
    "#         all_predictions.extend(predictions)\n",
    "    \n",
    "#     return all_row_ids, all_predictions\n",
    "\n",
    "# def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "#     \"\"\"Create submission dataframe\"\"\"\n",
    "#     print(\"Creating submission dataframe...\")\n",
    "\n",
    "#     submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "#     for i, species in enumerate(species_ids):\n",
    "#         submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "#     submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "#     submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "#     sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "#     missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "#     if missing_cols:\n",
    "#         print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "#         for col in missing_cols:\n",
    "#             submission_df[col] = 0.0\n",
    "\n",
    "#     submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "#     submission_df = submission_df.reset_index()\n",
    "    \n",
    "#     return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5347d70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:24.494416Z",
     "iopub.status.busy": "2025-05-27T03:34:24.494147Z",
     "iopub.status.idle": "2025-05-27T03:34:24.497235Z",
     "shell.execute_reply": "2025-05-27T03:34:24.496501Z"
    },
    "papermill": {
     "duration": 0.010659,
     "end_time": "2025-05-27T03:34:24.498410",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.487751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     start_time = time.time()\n",
    "#     print(\"Starting BirdCLEF-2025 inference...\")\n",
    "#     print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "#     models = load_models(cfg, num_classes)\n",
    "    \n",
    "#     if not models:\n",
    "#         print(\"No models found! Please check model paths.\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "#     row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "#     submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "#     submission_path = 'submission1.csv'\n",
    "#     submission_df.to_csv(submission_path, index=False)\n",
    "#     print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1c72ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:24.512037Z",
     "iopub.status.busy": "2025-05-27T03:34:24.511747Z",
     "iopub.status.idle": "2025-05-27T03:34:24.514790Z",
     "shell.execute_reply": "2025-05-27T03:34:24.514099Z"
    },
    "papermill": {
     "duration": 0.010756,
     "end_time": "2025-05-27T03:34:24.515906",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.505150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('submission1.csv')\n",
    "# cols = sub.columns[1:]\n",
    "# groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "# groups = groups.values\n",
    "# for group in np.unique(groups):\n",
    "#     sub_group = sub[group == groups]\n",
    "#     predictions = sub_group[cols].values\n",
    "#     new_predictions = predictions.copy()\n",
    "#     for i in range(1, predictions.shape[0]-1):\n",
    "#         new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "#     new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "#     new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "#     sub_group[cols] = new_predictions\n",
    "#     sub[group == groups] = sub_group\n",
    "# sub.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d1f167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:24.528330Z",
     "iopub.status.busy": "2025-05-27T03:34:24.528050Z",
     "iopub.status.idle": "2025-05-27T03:34:37.463577Z",
     "shell.execute_reply": "2025-05-27T03:34:37.462651Z"
    },
    "papermill": {
     "duration": 12.943551,
     "end_time": "2025-05-27T03:34:37.465232",
     "exception": false,
     "start_time": "2025-05-27T03:34:24.521681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-7afced610101>:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=cfg.device)\n",
      "100%|██████████| 1/1 [00:00<00:00, 171.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing audio file for soundscape_8358733\n",
      "✅ Saved submission1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 基于 sample_submission.csv 精准预测指定段落\n",
    "# ✅ 自动 fallback 到 train_soundscapes\n",
    "# ✅ 使用 CPU 推理，并对每个 row_id 精准定位\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CFG:\n",
    "    test_soundscapes = \"/kaggle/input/birdclef-2025/test_soundscapes\"\n",
    "    fallback_soundscapes = \"/kaggle/input/birdclef-2025/train_soundscapes\"\n",
    "    taxonomy_csv = \"/kaggle/input/birdclef-2025/taxonomy.csv\"\n",
    "    submission_csv = \"/kaggle/input/birdclef-2025/sample_submission.csv\"\n",
    "    model_path = \"/kaggle/input/bird25-d-330v2-ppv15-convnextv2-nano\"\n",
    "    model_name = \"convnextv2_nano.fcmae_ft_in22k_in1k\"\n",
    "\n",
    "    FS = 32000\n",
    "    WINDOW_SIZE = 5\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (128, 128)\n",
    "\n",
    "    folds = [0]\n",
    "    in_channels = 1\n",
    "    device = \"cpu\"\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "species_ids = pd.read_csv(cfg.taxonomy_csv)['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(cfg.model_name, pretrained=False, in_chans=cfg.in_channels)\n",
    "        out_dim = self.backbone.get_classifier().in_features\n",
    "        self.backbone.reset_classifier(0, '')\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(out_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features).view(features.size(0), -1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "def audio2melspec(audio, cfg):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=cfg.FS, n_fft=cfg.N_FFT, hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS, fmin=cfg.FMIN, fmax=cfg.FMAX\n",
    "    )\n",
    "    db = librosa.power_to_db(mel, ref=np.max)\n",
    "    norm = (db - db.min()) / (db.max() - db.min() + 1e-6)\n",
    "    return norm\n",
    "\n",
    "def process_segment(audio_seg, cfg):\n",
    "    if len(audio_seg) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        pad_len = cfg.FS * cfg.WINDOW_SIZE - len(audio_seg)\n",
    "        audio_seg = np.pad(audio_seg, (0, pad_len), mode='constant')\n",
    "    mel = audio2melspec(audio_seg, cfg)\n",
    "    mel = cv2.resize(mel, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "    return mel.astype(np.float32)\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    model_files = list(Path(cfg.model_path).glob(\"**/*fold0*.pth\"))\n",
    "    models = []\n",
    "    for path in model_files:\n",
    "        model = BirdCLEFModel(cfg, num_classes)\n",
    "        checkpoint = torch.load(path, map_location=cfg.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(cfg.device).eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def predict_clip(row_id, audio_data, cfg, model):\n",
    "    soundscape_id, end_time = row_id.rsplit(\"_\", 1)\n",
    "    end_time = int(end_time)\n",
    "    start_sample = (end_time - cfg.WINDOW_SIZE) * cfg.FS\n",
    "    end_sample = end_time * cfg.FS\n",
    "    seg = audio_data[start_sample:end_sample]\n",
    "    mel = process_segment(seg, cfg)\n",
    "    tensor = torch.tensor(mel).unsqueeze(0).unsqueeze(0).to(cfg.device)\n",
    "    with torch.no_grad():\n",
    "        prob = torch.sigmoid(model(tensor)).cpu().numpy().squeeze()\n",
    "    return row_id, prob\n",
    "\n",
    "def run_all(cfg):\n",
    "    model = load_models(cfg, num_classes)[0]\n",
    "    sample_df = pd.read_csv(cfg.submission_csv)\n",
    "    grouped = sample_df.groupby(sample_df['row_id'].str.rsplit('_', n=1).str[0])\n",
    "\n",
    "    row_ids, preds = [], []\n",
    "    for soundscape_id, group in tqdm(grouped):\n",
    "        for base in [cfg.test_soundscapes, cfg.fallback_soundscapes]:\n",
    "            audio_path = Path(base) / f\"{soundscape_id}.ogg\"\n",
    "            if audio_path.exists():\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Missing audio file for {soundscape_id}\")\n",
    "            continue\n",
    "\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        for row_id in group['row_id']:\n",
    "            rid, pred = predict_clip(row_id, audio_data, cfg, model)\n",
    "            row_ids.append(rid)\n",
    "            preds.append(pred)\n",
    "\n",
    "    return row_ids, preds\n",
    "\n",
    "def make_submission(row_ids, predictions, species_ids, cfg):\n",
    "    df = pd.DataFrame(predictions, columns=species_ids)\n",
    "    df.insert(0, 'row_id', row_ids)\n",
    "    sample = pd.read_csv(cfg.submission_csv)\n",
    "    for col in sample.columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    df = df[sample.columns]\n",
    "    df.to_csv(\"submission1.csv\", index=False)\n",
    "    print(\"✅ Saved submission1.csv\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    row_ids, preds = run_all(cfg)\n",
    "    make_submission(row_ids, preds, species_ids, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4da5b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:37.478447Z",
     "iopub.status.busy": "2025-05-27T03:34:37.478143Z",
     "iopub.status.idle": "2025-05-27T03:34:37.514656Z",
     "shell.execute_reply": "2025-05-27T03:34:37.513550Z"
    },
    "papermill": {
     "duration": 0.044262,
     "end_time": "2025-05-27T03:34:37.515946",
     "exception": false,
     "start_time": "2025-05-27T03:34:37.471684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, 1139490, 1192948, 1194042, 126247, 1346504, 134933, 135045, 1462711, 1462737, 1564122, 21038, 21116, 21211, 22333, 22973, 22976, 24272, 24292, 24322, 41663, 41778, 41970, 42007, 42087, 42113, 46010, 47067, 476537, 476538, 48124, 50186, 517119, 523060, 528041, 52884, 548639, 555086, 555142, 566513, 64862, 65336, 65344, 65349, 65373, 65419, 65448, 65547, 65962, 66016, 66531, 66578, 66893, 67082, 67252, 714022, 715170, 787625, 81930, 868458, 963335, amakin1, amekes, ampkin1, anhing, babwar, bafibi1, banana, baymac, bbwduc, bicwre1, bkcdon, bkmtou1, blbgra1, blbwre1, blcant4, blchaw1, blcjay1, blctit1, blhpar1, blkvul, bobfly1, bobher1, brtpar1, bubcur1, bubwre1, bucmot3, bugtan, butsal1, cargra1, cattyr, chbant1, chfmac1, cinbec1, cocher1, cocwoo1, colara1, colcha1, compau, compot1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 207 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5476eb2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:37.528775Z",
     "iopub.status.busy": "2025-05-27T03:34:37.528498Z",
     "iopub.status.idle": "2025-05-27T03:34:38.396170Z",
     "shell.execute_reply": "2025-05-27T03:34:38.395427Z"
    },
    "papermill": {
     "duration": 0.87552,
     "end_time": "2025-05-27T03:34:38.397610",
     "exception": false,
     "start_time": "2025-05-27T03:34:37.522090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e14c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.410881Z",
     "iopub.status.busy": "2025-05-27T03:34:38.410563Z",
     "iopub.status.idle": "2025-05-27T03:34:38.414926Z",
     "shell.execute_reply": "2025-05-27T03:34:38.413973Z"
    },
    "papermill": {
     "duration": 0.012093,
     "end_time": "2025-05-27T03:34:38.416174",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.404081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "\n",
    "    stage = 'train_bce'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_files = ['/kaggle/input/bird2025-sed-ckpt/sedmodel.pth'\n",
    "                  ]\n",
    " \n",
    "    model_name = 'seresnext26t_32x4d'  \n",
    "    pretrained = False\n",
    "    in_channels = 1\n",
    "\n",
    "    \n",
    "    SR = 32000\n",
    "    target_duration = 5\n",
    "    train_duration = 10\n",
    "    \n",
    "    \n",
    "    device = 'cpu'\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2c59538",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.429607Z",
     "iopub.status.busy": "2025-05-27T03:34:38.429348Z",
     "iopub.status.idle": "2025-05-27T03:34:38.436294Z",
     "shell.execute_reply": "2025-05-27T03:34:38.435445Z"
    },
    "papermill": {
     "duration": 0.014677,
     "end_time": "2025-05-27T03:34:38.437437",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.422760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "762f47a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.450858Z",
     "iopub.status.busy": "2025-05-27T03:34:38.450551Z",
     "iopub.status.idle": "2025-05-27T03:34:38.461132Z",
     "shell.execute_reply": "2025-05-27T03:34:38.460006Z"
    },
    "papermill": {
     "duration": 0.0187,
     "end_time": "2025-05-27T03:34:38.462530",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.443830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "696adb7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.477372Z",
     "iopub.status.busy": "2025-05-27T03:34:38.477100Z",
     "iopub.status.idle": "2025-05-27T03:34:38.483949Z",
     "shell.execute_reply": "2025-05-27T03:34:38.483167Z"
    },
    "papermill": {
     "duration": 0.015043,
     "end_time": "2025-05-27T03:34:38.485135",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.470092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "557eac45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.498822Z",
     "iopub.status.busy": "2025-05-27T03:34:38.498533Z",
     "iopub.status.idle": "2025-05-27T03:34:38.512441Z",
     "shell.execute_reply": "2025-05-27T03:34:38.511687Z"
    },
    "papermill": {
     "duration": 0.022142,
     "end_time": "2025-05-27T03:34:38.513843",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.491701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(cfg['n_mels'])\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg['model_name'],\n",
    "            pretrained=False,\n",
    "            in_chans=cfg['in_channels'],\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        if \"efficientnet\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if self.cfg['device'] == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_feature(self,x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # if self.training:\n",
    "        #    x = self.spec_augmenter(x)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "\n",
    "        audio = audio.float()\n",
    "        \n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "        if self.cfg['in_channels'] == 3:\n",
    "            spec = image_delta(spec)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "\n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        return torch.logit(clipwise_output)\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        x,_ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "        start = (\n",
    "            feat_time / 2 - feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train']) / 2\n",
    "        )\n",
    "        end = start + feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train'])\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        pred = self.attention_infer(start,end,x,time_att)\n",
    "\n",
    "        start_minus = max(0, start-tta_delta)\n",
    "        end_minus=end-tta_delta\n",
    "        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n",
    "\n",
    "        start_plus = start+tta_delta\n",
    "        end_plus=min(feat_time, end+tta_delta)\n",
    "        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n",
    "\n",
    "        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n",
    "        return pred\n",
    "        \n",
    "    def attention_infer(self,start,end,x,time_att):\n",
    "        feat = x[:, :, start:end]\n",
    "        # att = torch.softmax(time_att[:, :, start:end], dim=-1)\n",
    "        #             print(feat_time, start, end)\n",
    "        #             print(att_a.sum(), att.sum(), time_att.shape)\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        # clipwise_output = torch.sum(framewise_pred * att, dim=-1)\n",
    "        #logits = torch.sum(\n",
    "        #    self.att_block.cla(feat) * att,\n",
    "        #    dim=-1,\n",
    "        #)\n",
    "\n",
    "        # return clipwise_output\n",
    "        return framewise_pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4713046d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.527456Z",
     "iopub.status.busy": "2025-05-27T03:34:38.527133Z",
     "iopub.status.idle": "2025-05-27T03:34:38.533272Z",
     "shell.execute_reply": "2025-05-27T03:34:38.532563Z"
    },
    "papermill": {
     "duration": 0.014195,
     "end_time": "2025-05-27T03:34:38.534460",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.520265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sample(path, cfg):\n",
    "    audio, orig_sr = sf.read(path, dtype=\"float32\")\n",
    "    seconds = []\n",
    "    audio_length = cfg.SR * cfg.target_duration\n",
    "    step = audio_length\n",
    "    for i in range(audio_length, len(audio) + step, step):\n",
    "        start = max(0, i - audio_length)\n",
    "        end = start + audio_length\n",
    "        if end > len(audio):\n",
    "            pass\n",
    "        else:\n",
    "            seconds.append(int(end/cfg.SR))\n",
    "\n",
    "    audio = np.concatenate([audio,audio,audio])\n",
    "    audios = []\n",
    "    for i,second in enumerate(seconds):\n",
    "        end_seconds = int(second)\n",
    "        start_seconds = int(end_seconds - cfg.target_duration)\n",
    "    \n",
    "        end_index = int(cfg.SR * (end_seconds + (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        start_index = int(cfg.SR * (start_seconds - (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        end_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        start_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        y = audio[start_index:end_index].astype(np.float32)\n",
    "        if i==0:\n",
    "            y[:start_pad] = 0\n",
    "        elif i==(len(seconds)-1):\n",
    "            y[-end_pad:] = 0\n",
    "        audios.append(y)\n",
    "\n",
    "    return audios\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad24c019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.549148Z",
     "iopub.status.busy": "2025-05-27T03:34:38.548794Z",
     "iopub.status.idle": "2025-05-27T03:34:38.560008Z",
     "shell.execute_reply": "2025-05-27T03:34:38.558769Z"
    },
    "papermill": {
     "duration": 0.02101,
     "end_time": "2025-05-27T03:34:38.561883",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.540873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # model_files = find_model_files(cfg)\n",
    "    model_files = cfg.model_files\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for i, model_path in enumerate(model_files):\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n",
    "            cfg_temp = checkpoint['cfg']\n",
    "            cfg_temp['device'] = cfg.device\n",
    "            \n",
    "            model = BirdCLEFModel(cfg_temp)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            model.zero_grad()\n",
    "            model.half().float()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    print(f\"Processing {soundscape_id}\")\n",
    "    audio_data = load_sample(audio_path, cfg)\n",
    "    for segment_idx, audio_input in enumerate(audio_data):\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.target_duration\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "        \n",
    "        mel_spec = torch.tensor(audio_input, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        mel_spec = mel_spec.to(cfg.device)\n",
    "        \n",
    "        if len(models) == 1:\n",
    "            with torch.no_grad():\n",
    "                outputs = models[0].infer(mel_spec)\n",
    "                final_preds = outputs.squeeze()\n",
    "                # final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "\n",
    "        else:\n",
    "            segment_preds = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.infer(mel_spec)\n",
    "                    probs = outputs.squeeze()\n",
    "                    # probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                    segment_preds.append(probs)\n",
    "\n",
    "            \n",
    "            final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "        predictions.append(final_preds)\n",
    "\n",
    "    predictions = np.stack(predictions,axis=0)\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203e3c5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.577589Z",
     "iopub.status.busy": "2025-05-27T03:34:38.577198Z",
     "iopub.status.idle": "2025-05-27T03:34:38.589639Z",
     "shell.execute_reply": "2025-05-27T03:34:38.588601Z"
    },
    "papermill": {
     "duration": 0.022156,
     "end_time": "2025-05-27T03:34:38.591059",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.568903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:10]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(\n",
    "        executor.map(\n",
    "            predict_on_spectrogram,\n",
    "            test_files,\n",
    "            itertools.repeat(models),\n",
    "            itertools.repeat(cfg),\n",
    "            itertools.repeat(species_ids)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def smooth_submission(submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n",
    "        \n",
    "        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n",
    "        are averaged with those of its neighbors using defined weights.\n",
    "        \n",
    "        :param submission_path: Path to the submission CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        # Extract group names by splitting row_id on the last underscore\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Smooth the predictions using neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "            # Replace the smoothed values in the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54f72a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.606355Z",
     "iopub.status.busy": "2025-05-27T03:34:38.606052Z",
     "iopub.status.idle": "2025-05-27T03:34:38.611599Z",
     "shell.execute_reply": "2025-05-27T03:34:38.610863Z"
    },
    "papermill": {
     "duration": 0.014881,
     "end_time": "2025-05-27T03:34:38.613242",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.598361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission2.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    smooth_submission(submission_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c21032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:34:38.629591Z",
     "iopub.status.busy": "2025-05-27T03:34:38.629317Z",
     "iopub.status.idle": "2025-05-27T03:35:00.918693Z",
     "shell.execute_reply": "2025-05-27T03:35:00.917763Z"
    },
    "papermill": {
     "duration": 22.297482,
     "end_time": "2025-05-27T03:35:00.919927",
     "exception": false,
     "start_time": "2025-05-27T03:34:38.622445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "Found a total of 1 model files.\n",
      "Loading model: /kaggle/input/bird2025-sed-ckpt/sedmodel.pth\n",
      "Model usage: Single model\n",
      "Found 10 test soundscapes\n",
      "Processing H02_20230420_074000\n",
      "Processing H02_20230420_112000\n",
      "Processing H02_20230420_154500\n",
      "Processing H02_20230420_164000\n",
      "Processing H02_20230420_223500\n",
      "Processing H02_20230421_093000\n",
      "Processing H02_20230421_113500\n",
      "Processing H02_20230421_170000\n",
      "Processing H02_20230421_190500\n",
      "Processing H02_20230421_233500\n",
      "Creating submission dataframe...\n",
      "Submission saved to submission2.csv\n",
      "Smoothing submission predictions...\n",
      "Smoothed submission saved to submission2.csv\n",
      "Inference completed in 0.37 minutes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4610ca",
   "metadata": {
    "papermill": {
     "duration": 0.006224,
     "end_time": "2025-05-27T03:35:00.933144",
     "exception": false,
     "start_time": "2025-05-27T03:35:00.926920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Submission2(nfnet)》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d4aab0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:00.947132Z",
     "iopub.status.busy": "2025-05-27T03:35:00.946859Z",
     "iopub.status.idle": "2025-05-27T03:35:00.951582Z",
     "shell.execute_reply": "2025-05-27T03:35:00.950930Z"
    },
    "papermill": {
     "duration": 0.013017,
     "end_time": "2025-05-27T03:35:00.952723",
     "exception": false,
     "start_time": "2025-05-27T03:35:00.939706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "def apply_power_to_low_ranked_cols(\n",
    "    p: np.ndarray,\n",
    "    top_k: int = 30,\n",
    "    exponent: Union[int, float] = 2,\n",
    "    inplace: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rank columns by their column‑wise maximum and raise every column whose\n",
    "    rank falls below `top_k` to a given power.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "        A 2‑D array of shape **(n_chunks, n_classes)**.\n",
    "\n",
    "        - **n_chunks** is the number of fixed‑length time chunks obtained\n",
    "          after slicing the input audio (or other sequential data).  \n",
    "          *Example:* In the BirdCLEF `test_soundscapes` set, each file is\n",
    "          60 s long. If you extract non‑overlapping 5 s windows,  \n",
    "          `n_chunks = 60 s / 5 s = 12`.\n",
    "        - **n_classes** is the number of classes being predicted.\n",
    "        - Each element `p[i, j]` is the score or probability of class *j*\n",
    "          in chunk *i*.\n",
    "\n",
    "    top_k : int, default=30\n",
    "        The highest‑ranked columns (by their maximum value) that remain\n",
    "        unchanged.\n",
    "\n",
    "    exponent : int or float, default=2\n",
    "        The power applied to the selected low‑ranked columns  \n",
    "        (e.g. `2` squares, `0.5` takes the square root, `3` cubes).\n",
    "\n",
    "    inplace : bool, default=True\n",
    "        If `True`, modify `p` in place.  \n",
    "        If `False`, operate on a copy and leave the original array intact.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The transformed array. It is the same object as `p` when\n",
    "        `inplace=True`; otherwise, it is a new array.\n",
    "\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        p = p.copy()\n",
    "\n",
    "    # Identify columns whose max value ranks below `top_k`\n",
    "    tail_cols = np.argsort(-p.max(axis=0))[top_k:]\n",
    "\n",
    "    # Apply the power transformation to those columns\n",
    "    p[:, tail_cols] = p[:, tail_cols] ** exponent\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38d22a3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:00.966463Z",
     "iopub.status.busy": "2025-05-27T03:35:00.966240Z",
     "iopub.status.idle": "2025-05-27T03:35:00.969836Z",
     "shell.execute_reply": "2025-05-27T03:35:00.969182Z"
    },
    "papermill": {
     "duration": 0.011558,
     "end_time": "2025-05-27T03:35:00.971000",
     "exception": false,
     "start_time": "2025-05-27T03:35:00.959442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from contextlib import contextmanager\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f30fae07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:00.985074Z",
     "iopub.status.busy": "2025-05-27T03:35:00.984890Z",
     "iopub.status.idle": "2025-05-27T03:35:00.990432Z",
     "shell.execute_reply": "2025-05-27T03:35:00.989758Z"
    },
    "papermill": {
     "duration": 0.013651,
     "end_time": "2025-05-27T03:35:00.991683",
     "exception": false,
     "start_time": "2025-05-27T03:35:00.978032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: False\n",
      "Number of test soundscapes: 0\n"
     ]
    }
   ],
   "source": [
    "test_audio_dir = '../input/birdclef-2025/test_soundscapes/'\n",
    "file_list = [f for f in sorted(os.listdir(test_audio_dir))]\n",
    "file_list = [file.split('.')[0] for file in file_list if file.endswith('.ogg')]\n",
    "\n",
    "debug = False\n",
    "print('Debug mode:', debug)\n",
    "print('Number of test soundscapes:', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a67affc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:01.006428Z",
     "iopub.status.busy": "2025-05-27T03:35:01.006098Z",
     "iopub.status.idle": "2025-05-27T03:35:01.020583Z",
     "shell.execute_reply": "2025-05-27T03:35:01.019592Z"
    },
    "papermill": {
     "duration": 0.023335,
     "end_time": "2025-05-27T03:35:01.022099",
     "exception": false,
     "start_time": "2025-05-27T03:35:00.998764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wav_sec = 5\n",
    "sample_rate = 32000\n",
    "min_segment = sample_rate*wav_sec\n",
    "\n",
    "class_labels = sorted(os.listdir('../input/birdclef-2025/train_audio/'))\n",
    "\n",
    "n_fft=1024\n",
    "win_length=1024\n",
    "hop_length=512\n",
    "f_min=50\n",
    "f_max=16000\n",
    "n_mels=128\n",
    "\n",
    "mel_spectrogram = AT.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    f_min=f_min,\n",
    "    f_max=f_max,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    "    # normalized=True\n",
    ")\n",
    "\n",
    "def normalize_std(spec, eps=1e-6):\n",
    "    mean = torch.mean(spec)\n",
    "    std = torch.std(spec)\n",
    "    return torch.where(std == 0, spec-mean, (spec - mean) / (std+eps))\n",
    "\n",
    "def audio_to_mel(filepath=None):\n",
    "    waveform, sample_rate = torchaudio.load(filepath,backend=\"soundfile\")\n",
    "    len_wav = waveform.shape[1]\n",
    "    waveform = waveform[0,:].reshape(1, len_wav) # stereo->mono mono->mono\n",
    "    PREDS = []\n",
    "    for i in range(12):\n",
    "        waveform2 = waveform[:,i*sample_rate*5:i*sample_rate*5+sample_rate*5]\n",
    "        melspec = mel_spectrogram(waveform2)\n",
    "        melspec = torch.log(melspec+1e-6)\n",
    "        melspec = normalize_std(melspec)\n",
    "        melspec = torch.unsqueeze(melspec, dim=0)\n",
    "        \n",
    "        PREDS.append(melspec)\n",
    "    return torch.vstack(PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34ad9e0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:01.036038Z",
     "iopub.status.busy": "2025-05-27T03:35:01.035769Z",
     "iopub.status.idle": "2025-05-27T03:35:01.048368Z",
     "shell.execute_reply": "2025-05-27T03:35:01.047704Z"
    },
    "papermill": {
     "duration": 0.020854,
     "end_time": "2025-05-27T03:35:01.049751",
     "exception": false,
     "start_time": "2025-05-27T03:35:01.028897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1, n_mels=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block2 = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data.transpose(2,3)\n",
    "        x = torch.cat((x,x,x),1)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block2(x)\n",
    "        logit = torch.sum(norm_att * self.att_block2.cla(x), dim=2)\n",
    "\n",
    "        output_dict = {\n",
    "            'logit': logit,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd4b1de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:01.064174Z",
     "iopub.status.busy": "2025-05-27T03:35:01.063879Z",
     "iopub.status.idle": "2025-05-27T03:35:01.068829Z",
     "shell.execute_reply": "2025-05-27T03:35:01.068120Z"
    },
    "papermill": {
     "duration": 0.013001,
     "end_time": "2025-05-27T03:35:01.069933",
     "exception": false,
     "start_time": "2025-05-27T03:35:01.056932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/birdclef-2025-sed-models-p/sed0.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed1.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed2.pth']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name='eca_nfnet_l0'\n",
    "pretrained=False\n",
    "in_channels=3\n",
    "\n",
    "MODELS = [f'/kaggle/input/birdclef-2025-sed-models-p/sed{i}.pth' for i in range(3)]\n",
    "\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05822c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:01.083715Z",
     "iopub.status.busy": "2025-05-27T03:35:01.083431Z",
     "iopub.status.idle": "2025-05-27T03:35:04.542561Z",
     "shell.execute_reply": "2025-05-27T03:35:04.541726Z"
    },
    "papermill": {
     "duration": 3.467552,
     "end_time": "2025-05-27T03:35:04.544104",
     "exception": false,
     "start_time": "2025-05-27T03:35:01.076552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for path in MODELS:\n",
    "    model = TimmSED(base_model_name=base_model_name,\n",
    "               pretrained=pretrained,\n",
    "               num_classes=len(class_labels),\n",
    "               in_channels=in_channels,\n",
    "               n_mels=n_mels);\n",
    "    model.load_state_dict(torch.load(path, weights_only=True, map_location=torch.device('cpu')))\n",
    "    model.eval();\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "119b06c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:04.558547Z",
     "iopub.status.busy": "2025-05-27T03:35:04.558230Z",
     "iopub.status.idle": "2025-05-27T03:35:04.563375Z",
     "shell.execute_reply": "2025-05-27T03:35:04.562684Z"
    },
    "papermill": {
     "duration": 0.013087,
     "end_time": "2025-05-27T03:35:04.564333",
     "exception": false,
     "start_time": "2025-05-27T03:35:04.551246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(afile):    \n",
    "    global pred\n",
    "    path = test_audio_dir + afile + '.ogg'\n",
    "    with torch.inference_mode():\n",
    "        sig = audio_to_mel(path)\n",
    "        outputs = None\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            p = model(sig)\n",
    "            p = torch.sigmoid(p['logit']).detach().cpu().numpy() \n",
    "            p = apply_power_to_low_ranked_cols(p, top_k=30,exponent=2)\n",
    "            if outputs is None: outputs = p\n",
    "            else: outputs += p\n",
    "            \n",
    "        outputs /= len(models)\n",
    "        chunks = [[] for i in range(12)]\n",
    "        for i in range(len(chunks)):        \n",
    "            chunk_end_time = (i + 1) * 5\n",
    "            row_id = afile + '_' + str(chunk_end_time)\n",
    "            pred['row_id'].append(row_id)\n",
    "            bird_no = 0\n",
    "            for bird in class_labels:         \n",
    "                pred[bird].append(outputs[i,bird_no])\n",
    "                bird_no += 1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f56cf778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:04.577998Z",
     "iopub.status.busy": "2025-05-27T03:35:04.577670Z",
     "iopub.status.idle": "2025-05-27T03:35:04.582448Z",
     "shell.execute_reply": "2025-05-27T03:35:04.581426Z"
    },
    "papermill": {
     "duration": 0.012782,
     "end_time": "2025-05-27T03:35:04.583558",
     "exception": false,
     "start_time": "2025-05-27T03:35:04.570776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = {'row_id': []}\n",
    "for species_code in class_labels:\n",
    "    pred[species_code] = []\n",
    "    \n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    _ = list(executor.map(prediction, file_list))\n",
    "end_t = time.time()\n",
    "\n",
    "if debug == True:\n",
    "    print(700*(end_t - start)/60/debug_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f972f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:04.597296Z",
     "iopub.status.busy": "2025-05-27T03:35:04.596950Z",
     "iopub.status.idle": "2025-05-27T03:35:04.608932Z",
     "shell.execute_reply": "2025-05-27T03:35:04.608121Z"
    },
    "papermill": {
     "duration": 0.020038,
     "end_time": "2025-05-27T03:35:04.609968",
     "exception": false,
     "start_time": "2025-05-27T03:35:04.589930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, 1139490, 1192948, 1194042, 126247, 1346504, 134933, 135045, 1462711, 1462737, 1564122, 21038, 21116, 21211, 22333, 22973, 22976, 24272, 24292, 24322, 41663, 41778, 41970, 42007, 42087, 42113, 46010, 47067, 476537, 476538, 48124, 50186, 517119, 523060, 528041, 52884, 548639, 555086, 555142, 566513, 64862, 65336, 65344, 65349, 65373, 65419, 65448, 65547, 65962, 66016, 66531, 66578, 66893, 67082, 67252, 714022, 715170, 787625, 81930, 868458, 963335, amakin1, amekes, ampkin1, anhing, babwar, bafibi1, banana, baymac, bbwduc, bicwre1, bkcdon, bkmtou1, blbgra1, blbwre1, blcant4, blchaw1, blcjay1, blctit1, blhpar1, blkvul, bobfly1, bobher1, brtpar1, bubcur1, bubwre1, bucmot3, bugtan, butsal1, cargra1, cattyr, chbant1, chfmac1, cinbec1, cocher1, cocwoo1, colara1, colcha1, compau, compot1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 207 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(pred, columns = ['row_id'] + class_labels) \n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "857c0570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:04.623713Z",
     "iopub.status.busy": "2025-05-27T03:35:04.623399Z",
     "iopub.status.idle": "2025-05-27T03:35:04.644258Z",
     "shell.execute_reply": "2025-05-27T03:35:04.643513Z"
    },
    "papermill": {
     "duration": 0.028821,
     "end_time": "2025-05-27T03:35:04.645272",
     "exception": false,
     "start_time": "2025-05-27T03:35:04.616451",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"submission3.csv\", index=False)    \n",
    "\n",
    "sub = pd.read_csv('submission3.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "\n",
    "if debug:\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17b5f5",
   "metadata": {
    "papermill": {
     "duration": 0.006074,
     "end_time": "2025-05-27T03:35:04.658223",
     "exception": false,
     "start_time": "2025-05-27T03:35:04.652149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Finaly Blending》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c80b35d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:04.671883Z",
     "iopub.status.busy": "2025-05-27T03:35:04.671537Z",
     "iopub.status.idle": "2025-05-27T03:35:04.674798Z",
     "shell.execute_reply": "2025-05-27T03:35:04.674080Z"
    },
    "papermill": {
     "duration": 0.011255,
     "end_time": "2025-05-27T03:35:04.675729",
     "exception": false,
     "start_time": "2025-05-27T03:35:04.664474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------- #\n",
    "# [IMPORTANT]\n",
    "# * Blending Weight\n",
    "# ------------------------------------------ #\n",
    "sub_w=[0.15,0.3, 0.55] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea3e72e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T03:35:04.689566Z",
     "iopub.status.busy": "2025-05-27T03:35:04.689217Z",
     "iopub.status.idle": "2025-05-27T03:35:06.825440Z",
     "shell.execute_reply": "2025-05-27T03:35:06.824237Z"
    },
    "papermill": {
     "duration": 2.144888,
     "end_time": "2025-05-27T03:35:06.826991",
     "exception": false,
     "start_time": "2025-05-27T03:35:04.682103",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Blended submission saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 权重设置\n",
    "sub_w = [0.15, 0.65, 0.20]  # 三个模型对应的权重，必须和下面的 submission 文件一一对应\n",
    "submission_paths = [\n",
    "    \"/kaggle/working/submission1.csv\",\n",
    "    \"/kaggle/working/submission2.csv\",\n",
    "    \"/kaggle/working/submission3.csv\"\n",
    "]\n",
    "\n",
    "# 加载所有 submission\n",
    "dfs = [pd.read_csv(path) for path in submission_paths]\n",
    "species_cols = [col for col in dfs[0].columns if col != 'row_id']\n",
    "\n",
    "# 重命名列：每列加编号后缀\n",
    "for i, df in enumerate(dfs):\n",
    "    dfs[i] = df.rename(columns={col: f\"{col} {i}\" for col in species_cols})\n",
    "\n",
    "# 依次 merge\n",
    "merged_df = dfs[0]\n",
    "for i in range(1, len(dfs)):\n",
    "    merged_df = pd.merge(merged_df, dfs[i], on=\"row_id\")\n",
    "\n",
    "# 融合预测\n",
    "for col in species_cols:\n",
    "    merged_df[col] = sum(merged_df[f\"{col} {i}\"] * sub_w[i] for i in range(len(dfs)))\n",
    "\n",
    "# 删除临时列\n",
    "for col in species_cols:\n",
    "    for i in range(len(dfs)):\n",
    "        del merged_df[f\"{col} {i}\"]\n",
    "\n",
    "# 保存最终提交文件\n",
    "merged_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ Blended submission saved as submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7430593,
     "sourceId": 11828260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7457365,
     "sourceId": 11867185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 47.821869,
   "end_time": "2025-05-27T03:35:09.653791",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-27T03:34:21.831922",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
