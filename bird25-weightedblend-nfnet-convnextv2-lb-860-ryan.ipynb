{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecef775",
   "metadata": {
    "papermill": {
     "duration": 0.008864,
     "end_time": "2025-05-24T05:03:32.148867",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.140003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Only Submission(LoadLocalTrainModel)\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c92176",
   "metadata": {
    "papermill": {
     "duration": 0.007583,
     "end_time": "2025-05-24T05:03:32.164780",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.157197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **ℹ️INFO**\n",
    "* This notebook is an weighted blend(nfnet * 0.6 + convnextv2 * 0.4).\n",
    "    * **GreatWork LB.850(nfnet)** https://www.kaggle.com/code/myso1987/post-processing-with-power-adjustment-for-low-rank\n",
    "    * **MyLocalTrainModel(convnextv2)** https://www.kaggle.com/datasets/hideyukizushi/bird25-d-330v2-ppv15-convnextv2-nano/data\n",
    "\n",
    "### **ℹ️WeightedBlend**\n",
    "* In CV tasks, it is common to adopt diverse backbones to ensure robustness, and I am sharing this in this competition because it is connected to the LB boost.\n",
    "* P.S. Adjusting the blending weights should make it fit LB better and improve the score. In that case, it is redundant, so I recommend submitting in a private notebook rather than a public notebook.\n",
    "\n",
    "### **ℹ️Appendix**\n",
    "* My old LB.829 notebook from this competition\n",
    "* https://www.kaggle.com/code/hideyukizushi/bird25-onlyinf-v2-s-focallossbce-cv-962-lb-829\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e139b855",
   "metadata": {
    "papermill": {
     "duration": 0.007717,
     "end_time": "2025-05-24T05:03:32.180560",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.172843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Submission1(convnextv2)》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85a1d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:32.197831Z",
     "iopub.status.busy": "2025-05-24T05:03:32.197418Z",
     "iopub.status.idle": "2025-05-24T05:03:32.203051Z",
     "shell.execute_reply": "2025-05-24T05:03:32.202078Z"
    },
    "papermill": {
     "duration": 0.016327,
     "end_time": "2025-05-24T05:03:32.204967",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.188640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "# import warnings\n",
    "# import logging\n",
    "# import time\n",
    "# import math\n",
    "# import cv2\n",
    "# from pathlib import Path\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import librosa\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import timm\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# logging.basicConfig(level=logging.ERROR)\n",
    "# print(\"Finished import group000000000 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01efd8fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:32.222511Z",
     "iopub.status.busy": "2025-05-24T05:03:32.222149Z",
     "iopub.status.idle": "2025-05-24T05:03:32.226298Z",
     "shell.execute_reply": "2025-05-24T05:03:32.225338Z"
    },
    "papermill": {
     "duration": 0.01457,
     "end_time": "2025-05-24T05:03:32.227883",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.213313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CFG:\n",
    " \n",
    "#     test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "#     submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "#     taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    \n",
    "#     # ------------------------------------------- #\n",
    "#     # [IMPORTANT]\n",
    "#     # * Melspectrogram & Audio Params\n",
    "#     # ------------------------------------------- #\n",
    "#     FS = 32000  \n",
    "#     WINDOW_SIZE = 5\n",
    "#     N_FFT = 2048\n",
    "#     HOP_LENGTH = 512\n",
    "#     N_MELS = 512\n",
    "#     FMIN = 20\n",
    "#     FMAX = 16000\n",
    "#     TARGET_SHAPE = (256, 256)\n",
    "\n",
    "#     # ------------------------------------------- #\n",
    "#     # * Model def\n",
    "#     # ------------------------------------------- #\n",
    "#     model_path = '/kaggle/input/bird25-d-330v2-ppv15-convnextv2-nano'\n",
    "#     model_name = 'convnextv2_nano.fcmae_ft_in22k_in1k'\n",
    "#     use_specific_folds = True\n",
    "#     folds = [0,1]\n",
    "#     in_channels = 1\n",
    "#     device = 'cpu'  \n",
    "    \n",
    "#     # Inference parameters\n",
    "#     batch_size = 16\n",
    "#     use_tta = False  \n",
    "#     tta_count = 3\n",
    "#     threshold = 0.5\n",
    "\n",
    "#     # util\n",
    "#     debug = False\n",
    "#     debug_count = 3\n",
    "\n",
    "# cfg = CFG()\n",
    "\n",
    "# print(f\"Using device: {cfg.device}\")\n",
    "# print(f\"Loading taxonomy data...\")\n",
    "# taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "# species_ids = taxonomy_df['primary_label'].tolist()\n",
    "# num_classes = len(species_ids)\n",
    "# print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2d0f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:32.245389Z",
     "iopub.status.busy": "2025-05-24T05:03:32.245068Z",
     "iopub.status.idle": "2025-05-24T05:03:32.249589Z",
     "shell.execute_reply": "2025-05-24T05:03:32.248440Z"
    },
    "papermill": {
     "duration": 0.01524,
     "end_time": "2025-05-24T05:03:32.251430",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.236190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class GeM(nn.Module):\n",
    "#     def __init__(self, p=3, eps=1e-6):\n",
    "#         super(GeM, self).__init__()\n",
    "#         self.p = nn.Parameter(torch.ones(1)*p)\n",
    "#         self.eps = eps\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "#     def gem(self, x, p=3, eps=1e-6):\n",
    "#         return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + \\\n",
    "#                 '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "#                 ', ' + 'eps=' + str(self.eps) + ')'\n",
    "# class BirdCLEFModel(nn.Module):\n",
    "#     def __init__(self, cfg, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "        \n",
    "#         self.backbone = timm.create_model(\n",
    "#             cfg.model_name,\n",
    "#             pretrained=False,  \n",
    "#             in_chans=cfg.in_channels,\n",
    "#             drop_rate=0.0,    \n",
    "#             drop_path_rate=0.0\n",
    "#         )\n",
    "        \n",
    "#         if 'efficientnet' in cfg.model_name:\n",
    "#             backbone_out = self.backbone.classifier.in_features\n",
    "#             self.backbone.classifier = nn.Identity()\n",
    "#         elif 'resnet' in cfg.model_name:\n",
    "#             backbone_out = self.backbone.fc.in_features\n",
    "#             self.backbone.fc = nn.Identity()\n",
    "#         else:\n",
    "#             backbone_out = self.backbone.get_classifier().in_features\n",
    "#             self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "#         self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.feat_dim = backbone_out\n",
    "#         self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         features = self.backbone(x)\n",
    "        \n",
    "#         if isinstance(features, dict):\n",
    "#             features = features['features']\n",
    "            \n",
    "#         if len(features.shape) == 4:\n",
    "#             features = self.pooling(features)\n",
    "#             features = features.view(features.size(0), -1)\n",
    "        \n",
    "#         logits = self.classifier(features)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bcd339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:32.269020Z",
     "iopub.status.busy": "2025-05-24T05:03:32.268619Z",
     "iopub.status.idle": "2025-05-24T05:03:32.274669Z",
     "shell.execute_reply": "2025-05-24T05:03:32.273601Z"
    },
    "papermill": {
     "duration": 0.016802,
     "end_time": "2025-05-24T05:03:32.276502",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.259700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def audio2melspec(audio_data, cfg):\n",
    "#     \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "#     if np.isnan(audio_data).any():\n",
    "#         mean_signal = np.nanmean(audio_data)\n",
    "#         audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "#     mel_spec = librosa.feature.melspectrogram(\n",
    "#         y=audio_data,\n",
    "#         sr=cfg.FS,\n",
    "#         n_fft=cfg.N_FFT,\n",
    "#         hop_length=cfg.HOP_LENGTH,\n",
    "#         n_mels=cfg.N_MELS,\n",
    "#         fmin=cfg.FMIN,\n",
    "#         fmax=cfg.FMAX,\n",
    "#         power=2.0,\n",
    "#         pad_mode=\"reflect\",\n",
    "#         norm='slaney',\n",
    "#         htk=True,\n",
    "#         center=True,\n",
    "#     )\n",
    "\n",
    "#     mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "#     mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "#     return mel_spec_norm\n",
    "\n",
    "# def process_audio_segment(audio_data, cfg):\n",
    "#     \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "#     if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "#         audio_data = np.pad(audio_data, \n",
    "#                           (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "#                           mode='constant')\n",
    "    \n",
    "#     mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "#     # Resize if needed\n",
    "#     if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "#         mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "#     return mel_spec.astype(np.float32)\n",
    "    \n",
    "# def find_model_files(cfg):\n",
    "#     \"\"\"\n",
    "#     Find all .pth model files in the specified model directory\n",
    "#     \"\"\"\n",
    "#     model_files = []\n",
    "    \n",
    "#     model_dir = Path(cfg.model_path)\n",
    "    \n",
    "#     for path in model_dir.glob('**/*.pth'):\n",
    "#         model_files.append(str(path))\n",
    "    \n",
    "#     return model_files\n",
    "\n",
    "# def load_models(cfg, num_classes):\n",
    "#     \"\"\"\n",
    "#     Load all found model files and prepare them for ensemble\n",
    "#     \"\"\"\n",
    "#     models = []\n",
    "    \n",
    "#     model_files = find_model_files(cfg)\n",
    "    \n",
    "#     if not model_files:\n",
    "#         print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "#         return models\n",
    "    \n",
    "#     print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "#     if cfg.use_specific_folds:\n",
    "#         filtered_files = []\n",
    "#         for fold in cfg.folds:\n",
    "#             fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "#             filtered_files.extend(fold_files)\n",
    "#         model_files = filtered_files\n",
    "#         print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
    "    \n",
    "#     for model_path in model_files:\n",
    "#         try:\n",
    "#             print(f\"Loading model: {model_path}\")\n",
    "#             checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n",
    "            \n",
    "#             model = BirdCLEFModel(cfg, num_classes)\n",
    "#             model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#             model = model.to(cfg.device)\n",
    "#             model.eval()\n",
    "            \n",
    "#             models.append(model)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "#     return models\n",
    "\n",
    "# def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "#     \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "#     predictions = []\n",
    "#     row_ids = []\n",
    "#     soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"Processing {soundscape_id}\")\n",
    "#         audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "#         total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "#         for segment_idx in range(total_segments):\n",
    "#             start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "#             end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "#             segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "#             end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "#             row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "#             row_ids.append(row_id)\n",
    "\n",
    "#             if cfg.use_tta:\n",
    "#                 all_preds = []\n",
    "                \n",
    "#                 for tta_idx in range(cfg.tta_count):\n",
    "#                     mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "#                     mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "#                     mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "#                     mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "#                     if len(models) == 1:\n",
    "#                         with torch.no_grad():\n",
    "#                             outputs = models[0](mel_spec)\n",
    "#                             probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                             all_preds.append(probs)\n",
    "#                     else:\n",
    "#                         segment_preds = []\n",
    "#                         for model in models:\n",
    "#                             with torch.no_grad():\n",
    "#                                 outputs = model(mel_spec)\n",
    "#                                 probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                                 segment_preds.append(probs)\n",
    "                        \n",
    "#                         avg_preds = np.mean(segment_preds, axis=0)\n",
    "#                         all_preds.append(avg_preds)\n",
    "\n",
    "#                 final_preds = np.mean(all_preds, axis=0)\n",
    "#             else:\n",
    "#                 mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "#                 mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "#                 mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "#                 if len(models) == 1:\n",
    "#                     with torch.no_grad():\n",
    "#                         outputs = models[0](mel_spec)\n",
    "#                         final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                 else:\n",
    "#                     segment_preds = []\n",
    "#                     for model in models:\n",
    "#                         with torch.no_grad():\n",
    "#                             outputs = model(mel_spec)\n",
    "#                             probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                             segment_preds.append(probs)\n",
    "\n",
    "#                     final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "#             predictions.append(final_preds)\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {audio_path}: {e}\")\n",
    "    \n",
    "#     return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef01738f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:32.294111Z",
     "iopub.status.busy": "2025-05-24T05:03:32.293691Z",
     "iopub.status.idle": "2025-05-24T05:03:32.298366Z",
     "shell.execute_reply": "2025-05-24T05:03:32.297179Z"
    },
    "papermill": {
     "duration": 0.015391,
     "end_time": "2025-05-24T05:03:32.300205",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.284814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def apply_tta(spec, tta_idx):\n",
    "#     \"\"\"Apply test-time augmentation\"\"\"\n",
    "#     if tta_idx == 0:\n",
    "#         # Original spectrogram\n",
    "#         return spec\n",
    "#     elif tta_idx == 1:\n",
    "#         # Time shift (horizontal flip)\n",
    "#         return np.flip(spec, axis=1)\n",
    "#     elif tta_idx == 2:\n",
    "#         # Frequency shift (vertical flip)\n",
    "#         return np.flip(spec, axis=0)\n",
    "#     else:\n",
    "#         return spec\n",
    "\n",
    "# def run_inference(cfg, models, species_ids):\n",
    "#     \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "#     test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "#     if cfg.debug:\n",
    "#         print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "#         test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "#     print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "#     all_row_ids = []\n",
    "#     all_predictions = []\n",
    "\n",
    "#     for audio_path in tqdm(test_files):\n",
    "#         row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "#         all_row_ids.extend(row_ids)\n",
    "#         all_predictions.extend(predictions)\n",
    "    \n",
    "#     return all_row_ids, all_predictions\n",
    "\n",
    "# def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "#     \"\"\"Create submission dataframe\"\"\"\n",
    "#     print(\"Creating submission dataframe...\")\n",
    "\n",
    "#     submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "#     for i, species in enumerate(species_ids):\n",
    "#         submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "#     submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "#     submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "#     sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "#     missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "#     if missing_cols:\n",
    "#         print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "#         for col in missing_cols:\n",
    "#             submission_df[col] = 0.0\n",
    "\n",
    "#     submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "#     submission_df = submission_df.reset_index()\n",
    "    \n",
    "#     return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f641e648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:32.317886Z",
     "iopub.status.busy": "2025-05-24T05:03:32.317481Z",
     "iopub.status.idle": "2025-05-24T05:03:32.321787Z",
     "shell.execute_reply": "2025-05-24T05:03:32.320499Z"
    },
    "papermill": {
     "duration": 0.015324,
     "end_time": "2025-05-24T05:03:32.323659",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.308335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     start_time = time.time()\n",
    "#     print(\"Starting BirdCLEF-2025 inference...\")\n",
    "#     print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "#     models = load_models(cfg, num_classes)\n",
    "    \n",
    "#     if not models:\n",
    "#         print(\"No models found! Please check model paths.\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "#     row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "#     submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "#     submission_path = 'submission1.csv'\n",
    "#     submission_df.to_csv(submission_path, index=False)\n",
    "#     print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8863b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:32.341295Z",
     "iopub.status.busy": "2025-05-24T05:03:32.340882Z",
     "iopub.status.idle": "2025-05-24T05:03:32.344719Z",
     "shell.execute_reply": "2025-05-24T05:03:32.343646Z"
    },
    "papermill": {
     "duration": 0.014501,
     "end_time": "2025-05-24T05:03:32.346439",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.331938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('submission1.csv')\n",
    "# cols = sub.columns[1:]\n",
    "# groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "# groups = groups.values\n",
    "# for group in np.unique(groups):\n",
    "#     sub_group = sub[group == groups]\n",
    "#     predictions = sub_group[cols].values\n",
    "#     new_predictions = predictions.copy()\n",
    "#     for i in range(1, predictions.shape[0]-1):\n",
    "#         new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "#     new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "#     new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "#     sub_group[cols] = new_predictions\n",
    "#     sub[group == groups] = sub_group\n",
    "# sub.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5558702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:32.364173Z",
     "iopub.status.busy": "2025-05-24T05:03:32.363522Z",
     "iopub.status.idle": "2025-05-24T05:03:49.197709Z",
     "shell.execute_reply": "2025-05-24T05:03:49.196488Z"
    },
    "papermill": {
     "duration": 16.845527,
     "end_time": "2025-05-24T05:03:49.200132",
     "exception": false,
     "start_time": "2025-05-24T05:03:32.354605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1b63fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.218087Z",
     "iopub.status.busy": "2025-05-24T05:03:49.217693Z",
     "iopub.status.idle": "2025-05-24T05:03:49.223337Z",
     "shell.execute_reply": "2025-05-24T05:03:49.221984Z"
    },
    "papermill": {
     "duration": 0.016548,
     "end_time": "2025-05-24T05:03:49.225119",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.208571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "\n",
    "    stage = 'train_bce'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_files = ['/kaggle/input/bird2025-sed-ckpt/sedmodel.pth'\n",
    "                  ]\n",
    " \n",
    "    model_name = 'seresnext26t_32x4d'  \n",
    "    pretrained = False\n",
    "    in_channels = 1\n",
    "\n",
    "    \n",
    "    SR = 32000\n",
    "    target_duration = 5\n",
    "    train_duration = 10\n",
    "    \n",
    "    \n",
    "    device = 'cpu'\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c836890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.242975Z",
     "iopub.status.busy": "2025-05-24T05:03:49.242564Z",
     "iopub.status.idle": "2025-05-24T05:03:49.268393Z",
     "shell.execute_reply": "2025-05-24T05:03:49.266940Z"
    },
    "papermill": {
     "duration": 0.037168,
     "end_time": "2025-05-24T05:03:49.270486",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.233318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3f34ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.289093Z",
     "iopub.status.busy": "2025-05-24T05:03:49.288644Z",
     "iopub.status.idle": "2025-05-24T05:03:49.302069Z",
     "shell.execute_reply": "2025-05-24T05:03:49.301007Z"
    },
    "papermill": {
     "duration": 0.024537,
     "end_time": "2025-05-24T05:03:49.303855",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.279318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f11f22d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.321636Z",
     "iopub.status.busy": "2025-05-24T05:03:49.321287Z",
     "iopub.status.idle": "2025-05-24T05:03:49.330402Z",
     "shell.execute_reply": "2025-05-24T05:03:49.329158Z"
    },
    "papermill": {
     "duration": 0.019882,
     "end_time": "2025-05-24T05:03:49.332212",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.312330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9875a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.350384Z",
     "iopub.status.busy": "2025-05-24T05:03:49.349982Z",
     "iopub.status.idle": "2025-05-24T05:03:49.371867Z",
     "shell.execute_reply": "2025-05-24T05:03:49.370510Z"
    },
    "papermill": {
     "duration": 0.03313,
     "end_time": "2025-05-24T05:03:49.373831",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.340701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(cfg['n_mels'])\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg['model_name'],\n",
    "            pretrained=False,\n",
    "            in_chans=cfg['in_channels'],\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        if \"efficientnet\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if self.cfg['device'] == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_feature(self,x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # if self.training:\n",
    "        #    x = self.spec_augmenter(x)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "\n",
    "        audio = audio.float()\n",
    "        \n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "        if self.cfg['in_channels'] == 3:\n",
    "            spec = image_delta(spec)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "\n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        return torch.logit(clipwise_output)\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        x,_ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "        start = (\n",
    "            feat_time / 2 - feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train']) / 2\n",
    "        )\n",
    "        end = start + feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train'])\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        pred = self.attention_infer(start,end,x,time_att)\n",
    "\n",
    "        start_minus = max(0, start-tta_delta)\n",
    "        end_minus=end-tta_delta\n",
    "        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n",
    "\n",
    "        start_plus = start+tta_delta\n",
    "        end_plus=min(feat_time, end+tta_delta)\n",
    "        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n",
    "\n",
    "        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n",
    "        return pred\n",
    "        \n",
    "    def attention_infer(self,start,end,x,time_att):\n",
    "        feat = x[:, :, start:end]\n",
    "        # att = torch.softmax(time_att[:, :, start:end], dim=-1)\n",
    "        #             print(feat_time, start, end)\n",
    "        #             print(att_a.sum(), att.sum(), time_att.shape)\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        # clipwise_output = torch.sum(framewise_pred * att, dim=-1)\n",
    "        #logits = torch.sum(\n",
    "        #    self.att_block.cla(feat) * att,\n",
    "        #    dim=-1,\n",
    "        #)\n",
    "\n",
    "        # return clipwise_output\n",
    "        return framewise_pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d150d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.391868Z",
     "iopub.status.busy": "2025-05-24T05:03:49.391497Z",
     "iopub.status.idle": "2025-05-24T05:03:49.400146Z",
     "shell.execute_reply": "2025-05-24T05:03:49.398930Z"
    },
    "papermill": {
     "duration": 0.019839,
     "end_time": "2025-05-24T05:03:49.402054",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.382215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sample(path, cfg):\n",
    "    audio, orig_sr = sf.read(path, dtype=\"float32\")\n",
    "    seconds = []\n",
    "    audio_length = cfg.SR * cfg.target_duration\n",
    "    step = audio_length\n",
    "    for i in range(audio_length, len(audio) + step, step):\n",
    "        start = max(0, i - audio_length)\n",
    "        end = start + audio_length\n",
    "        if end > len(audio):\n",
    "            pass\n",
    "        else:\n",
    "            seconds.append(int(end/cfg.SR))\n",
    "\n",
    "    audio = np.concatenate([audio,audio,audio])\n",
    "    audios = []\n",
    "    for i,second in enumerate(seconds):\n",
    "        end_seconds = int(second)\n",
    "        start_seconds = int(end_seconds - cfg.target_duration)\n",
    "    \n",
    "        end_index = int(cfg.SR * (end_seconds + (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        start_index = int(cfg.SR * (start_seconds - (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        end_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        start_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        y = audio[start_index:end_index].astype(np.float32)\n",
    "        if i==0:\n",
    "            y[:start_pad] = 0\n",
    "        elif i==(len(seconds)-1):\n",
    "            y[-end_pad:] = 0\n",
    "        audios.append(y)\n",
    "\n",
    "    return audios\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5b60f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.419713Z",
     "iopub.status.busy": "2025-05-24T05:03:49.419371Z",
     "iopub.status.idle": "2025-05-24T05:03:49.431512Z",
     "shell.execute_reply": "2025-05-24T05:03:49.430444Z"
    },
    "papermill": {
     "duration": 0.023328,
     "end_time": "2025-05-24T05:03:49.433583",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.410255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # model_files = find_model_files(cfg)\n",
    "    model_files = cfg.model_files\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for i, model_path in enumerate(model_files):\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n",
    "            cfg_temp = checkpoint['cfg']\n",
    "            cfg_temp['device'] = cfg.device\n",
    "            \n",
    "            model = BirdCLEFModel(cfg_temp)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            model.zero_grad()\n",
    "            model.half().float()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    print(f\"Processing {soundscape_id}\")\n",
    "    audio_data = load_sample(audio_path, cfg)\n",
    "    for segment_idx, audio_input in enumerate(audio_data):\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.target_duration\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "        \n",
    "        mel_spec = torch.tensor(audio_input, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        mel_spec = mel_spec.to(cfg.device)\n",
    "        \n",
    "        if len(models) == 1:\n",
    "            with torch.no_grad():\n",
    "                outputs = models[0].infer(mel_spec)\n",
    "                final_preds = outputs.squeeze()\n",
    "                # final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "\n",
    "        else:\n",
    "            segment_preds = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.infer(mel_spec)\n",
    "                    probs = outputs.squeeze()\n",
    "                    # probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                    segment_preds.append(probs)\n",
    "\n",
    "            \n",
    "            final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "        predictions.append(final_preds)\n",
    "\n",
    "    predictions = np.stack(predictions,axis=0)\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e143669b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.451498Z",
     "iopub.status.busy": "2025-05-24T05:03:49.451150Z",
     "iopub.status.idle": "2025-05-24T05:03:49.464144Z",
     "shell.execute_reply": "2025-05-24T05:03:49.462815Z"
    },
    "papermill": {
     "duration": 0.024208,
     "end_time": "2025-05-24T05:03:49.466265",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.442057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:10]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(\n",
    "        executor.map(\n",
    "            predict_on_spectrogram,\n",
    "            test_files,\n",
    "            itertools.repeat(models),\n",
    "            itertools.repeat(cfg),\n",
    "            itertools.repeat(species_ids)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def smooth_submission(submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n",
    "        \n",
    "        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n",
    "        are averaged with those of its neighbors using defined weights.\n",
    "        \n",
    "        :param submission_path: Path to the submission CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        # Extract group names by splitting row_id on the last underscore\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Smooth the predictions using neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "            # Replace the smoothed values in the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86ae688c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.484562Z",
     "iopub.status.busy": "2025-05-24T05:03:49.484173Z",
     "iopub.status.idle": "2025-05-24T05:03:49.490456Z",
     "shell.execute_reply": "2025-05-24T05:03:49.488956Z"
    },
    "papermill": {
     "duration": 0.017647,
     "end_time": "2025-05-24T05:03:49.492501",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.474854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission1.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    smooth_submission(submission_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c893f0e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:03:49.511236Z",
     "iopub.status.busy": "2025-05-24T05:03:49.510724Z",
     "iopub.status.idle": "2025-05-24T05:04:23.130524Z",
     "shell.execute_reply": "2025-05-24T05:04:23.129409Z"
    },
    "papermill": {
     "duration": 33.630984,
     "end_time": "2025-05-24T05:04:23.132365",
     "exception": false,
     "start_time": "2025-05-24T05:03:49.501381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "Found a total of 1 model files.\n",
      "Loading model: /kaggle/input/bird2025-sed-ckpt/sedmodel.pth\n",
      "Model usage: Single model\n",
      "Found 10 test soundscapes\n",
      "Processing H02_20230420_074000\n",
      "Processing H02_20230420_112000\n",
      "Processing H02_20230420_154500\n",
      "Processing H02_20230420_164000\n",
      "Processing H02_20230420_223500\n",
      "Processing H02_20230421_093000\n",
      "Processing H02_20230421_113500\n",
      "Processing H02_20230421_170000\n",
      "Processing H02_20230421_190500\n",
      "Processing H02_20230421_233500\n",
      "Creating submission dataframe...\n",
      "Submission saved to submission1.csv\n",
      "Smoothing submission predictions...\n",
      "Smoothed submission saved to submission1.csv\n",
      "Inference completed in 0.56 minutes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f647e",
   "metadata": {
    "papermill": {
     "duration": 0.008237,
     "end_time": "2025-05-24T05:04:23.149394",
     "exception": false,
     "start_time": "2025-05-24T05:04:23.141157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Submission2(nfnet)》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50415a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:23.168215Z",
     "iopub.status.busy": "2025-05-24T05:04:23.167707Z",
     "iopub.status.idle": "2025-05-24T05:04:23.174382Z",
     "shell.execute_reply": "2025-05-24T05:04:23.173047Z"
    },
    "papermill": {
     "duration": 0.018157,
     "end_time": "2025-05-24T05:04:23.176141",
     "exception": false,
     "start_time": "2025-05-24T05:04:23.157984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "def apply_power_to_low_ranked_cols(\n",
    "    p: np.ndarray,\n",
    "    top_k: int = 30,\n",
    "    exponent: Union[int, float] = 2,\n",
    "    inplace: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rank columns by their column‑wise maximum and raise every column whose\n",
    "    rank falls below `top_k` to a given power.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "        A 2‑D array of shape **(n_chunks, n_classes)**.\n",
    "\n",
    "        - **n_chunks** is the number of fixed‑length time chunks obtained\n",
    "          after slicing the input audio (or other sequential data).  \n",
    "          *Example:* In the BirdCLEF `test_soundscapes` set, each file is\n",
    "          60 s long. If you extract non‑overlapping 5 s windows,  \n",
    "          `n_chunks = 60 s / 5 s = 12`.\n",
    "        - **n_classes** is the number of classes being predicted.\n",
    "        - Each element `p[i, j]` is the score or probability of class *j*\n",
    "          in chunk *i*.\n",
    "\n",
    "    top_k : int, default=30\n",
    "        The highest‑ranked columns (by their maximum value) that remain\n",
    "        unchanged.\n",
    "\n",
    "    exponent : int or float, default=2\n",
    "        The power applied to the selected low‑ranked columns  \n",
    "        (e.g. `2` squares, `0.5` takes the square root, `3` cubes).\n",
    "\n",
    "    inplace : bool, default=True\n",
    "        If `True`, modify `p` in place.  \n",
    "        If `False`, operate on a copy and leave the original array intact.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The transformed array. It is the same object as `p` when\n",
    "        `inplace=True`; otherwise, it is a new array.\n",
    "\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        p = p.copy()\n",
    "\n",
    "    # Identify columns whose max value ranks below `top_k`\n",
    "    tail_cols = np.argsort(-p.max(axis=0))[top_k:]\n",
    "\n",
    "    # Apply the power transformation to those columns\n",
    "    p[:, tail_cols] = p[:, tail_cols] ** exponent\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b5598dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:23.194559Z",
     "iopub.status.busy": "2025-05-24T05:04:23.194229Z",
     "iopub.status.idle": "2025-05-24T05:04:23.199073Z",
     "shell.execute_reply": "2025-05-24T05:04:23.198034Z"
    },
    "papermill": {
     "duration": 0.015866,
     "end_time": "2025-05-24T05:04:23.200730",
     "exception": false,
     "start_time": "2025-05-24T05:04:23.184864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from contextlib import contextmanager\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79cf6014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:23.219890Z",
     "iopub.status.busy": "2025-05-24T05:04:23.219481Z",
     "iopub.status.idle": "2025-05-24T05:04:23.226146Z",
     "shell.execute_reply": "2025-05-24T05:04:23.225133Z"
    },
    "papermill": {
     "duration": 0.018177,
     "end_time": "2025-05-24T05:04:23.227820",
     "exception": false,
     "start_time": "2025-05-24T05:04:23.209643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: False\n",
      "Number of test soundscapes: 0\n"
     ]
    }
   ],
   "source": [
    "test_audio_dir = '../input/birdclef-2025/test_soundscapes/'\n",
    "file_list = [f for f in sorted(os.listdir(test_audio_dir))]\n",
    "file_list = [file.split('.')[0] for file in file_list if file.endswith('.ogg')]\n",
    "\n",
    "debug = False\n",
    "print('Debug mode:', debug)\n",
    "print('Number of test soundscapes:', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f14c2d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:23.247029Z",
     "iopub.status.busy": "2025-05-24T05:04:23.246634Z",
     "iopub.status.idle": "2025-05-24T05:04:23.261655Z",
     "shell.execute_reply": "2025-05-24T05:04:23.260542Z"
    },
    "papermill": {
     "duration": 0.026542,
     "end_time": "2025-05-24T05:04:23.263676",
     "exception": false,
     "start_time": "2025-05-24T05:04:23.237134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wav_sec = 5\n",
    "sample_rate = 32000\n",
    "min_segment = sample_rate*wav_sec\n",
    "\n",
    "class_labels = sorted(os.listdir('../input/birdclef-2025/train_audio/'))\n",
    "\n",
    "n_fft=1024\n",
    "win_length=1024\n",
    "hop_length=512\n",
    "f_min=50\n",
    "f_max=16000\n",
    "n_mels=128\n",
    "\n",
    "mel_spectrogram = AT.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    f_min=f_min,\n",
    "    f_max=f_max,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    "    # normalized=True\n",
    ")\n",
    "\n",
    "def normalize_std(spec, eps=1e-6):\n",
    "    mean = torch.mean(spec)\n",
    "    std = torch.std(spec)\n",
    "    return torch.where(std == 0, spec-mean, (spec - mean) / (std+eps))\n",
    "\n",
    "def audio_to_mel(filepath=None):\n",
    "    waveform, sample_rate = torchaudio.load(filepath,backend=\"soundfile\")\n",
    "    len_wav = waveform.shape[1]\n",
    "    waveform = waveform[0,:].reshape(1, len_wav) # stereo->mono mono->mono\n",
    "    PREDS = []\n",
    "    for i in range(12):\n",
    "        waveform2 = waveform[:,i*sample_rate*5:i*sample_rate*5+sample_rate*5]\n",
    "        melspec = mel_spectrogram(waveform2)\n",
    "        melspec = torch.log(melspec+1e-6)\n",
    "        melspec = normalize_std(melspec)\n",
    "        melspec = torch.unsqueeze(melspec, dim=0)\n",
    "        \n",
    "        PREDS.append(melspec)\n",
    "    return torch.vstack(PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d44c1832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:23.282352Z",
     "iopub.status.busy": "2025-05-24T05:04:23.282012Z",
     "iopub.status.idle": "2025-05-24T05:04:23.300525Z",
     "shell.execute_reply": "2025-05-24T05:04:23.299202Z"
    },
    "papermill": {
     "duration": 0.029829,
     "end_time": "2025-05-24T05:04:23.302326",
     "exception": false,
     "start_time": "2025-05-24T05:04:23.272497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1, n_mels=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block2 = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data.transpose(2,3)\n",
    "        x = torch.cat((x,x,x),1)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block2(x)\n",
    "        logit = torch.sum(norm_att * self.att_block2.cla(x), dim=2)\n",
    "\n",
    "        output_dict = {\n",
    "            'logit': logit,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "372a44bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:23.321088Z",
     "iopub.status.busy": "2025-05-24T05:04:23.320691Z",
     "iopub.status.idle": "2025-05-24T05:04:23.327773Z",
     "shell.execute_reply": "2025-05-24T05:04:23.326748Z"
    },
    "papermill": {
     "duration": 0.018266,
     "end_time": "2025-05-24T05:04:23.329620",
     "exception": false,
     "start_time": "2025-05-24T05:04:23.311354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/birdclef-2025-sed-models-p/sed0.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed1.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed2.pth']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name='eca_nfnet_l0'\n",
    "pretrained=False\n",
    "in_channels=3\n",
    "\n",
    "MODELS = [f'/kaggle/input/birdclef-2025-sed-models-p/sed{i}.pth' for i in range(3)]\n",
    "\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "320dc1b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:23.348545Z",
     "iopub.status.busy": "2025-05-24T05:04:23.348210Z",
     "iopub.status.idle": "2025-05-24T05:04:27.068177Z",
     "shell.execute_reply": "2025-05-24T05:04:27.067181Z"
    },
    "papermill": {
     "duration": 3.731724,
     "end_time": "2025-05-24T05:04:27.070367",
     "exception": false,
     "start_time": "2025-05-24T05:04:23.338643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for path in MODELS:\n",
    "    model = TimmSED(base_model_name=base_model_name,\n",
    "               pretrained=pretrained,\n",
    "               num_classes=len(class_labels),\n",
    "               in_channels=in_channels,\n",
    "               n_mels=n_mels);\n",
    "    model.load_state_dict(torch.load(path, weights_only=True, map_location=torch.device('cpu')))\n",
    "    model.eval();\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0cb2fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:27.090334Z",
     "iopub.status.busy": "2025-05-24T05:04:27.089953Z",
     "iopub.status.idle": "2025-05-24T05:04:27.097015Z",
     "shell.execute_reply": "2025-05-24T05:04:27.095915Z"
    },
    "papermill": {
     "duration": 0.019363,
     "end_time": "2025-05-24T05:04:27.099075",
     "exception": false,
     "start_time": "2025-05-24T05:04:27.079712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(afile):    \n",
    "    global pred\n",
    "    path = test_audio_dir + afile + '.ogg'\n",
    "    with torch.inference_mode():\n",
    "        sig = audio_to_mel(path)\n",
    "        outputs = None\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            p = model(sig)\n",
    "            p = torch.sigmoid(p['logit']).detach().cpu().numpy() \n",
    "            p = apply_power_to_low_ranked_cols(p, top_k=30,exponent=2)\n",
    "            if outputs is None: outputs = p\n",
    "            else: outputs += p\n",
    "            \n",
    "        outputs /= len(models)\n",
    "        chunks = [[] for i in range(12)]\n",
    "        for i in range(len(chunks)):        \n",
    "            chunk_end_time = (i + 1) * 5\n",
    "            row_id = afile + '_' + str(chunk_end_time)\n",
    "            pred['row_id'].append(row_id)\n",
    "            bird_no = 0\n",
    "            for bird in class_labels:         \n",
    "                pred[bird].append(outputs[i,bird_no])\n",
    "                bird_no += 1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfb4527a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:27.118352Z",
     "iopub.status.busy": "2025-05-24T05:04:27.117996Z",
     "iopub.status.idle": "2025-05-24T05:04:27.123468Z",
     "shell.execute_reply": "2025-05-24T05:04:27.122438Z"
    },
    "papermill": {
     "duration": 0.016891,
     "end_time": "2025-05-24T05:04:27.125191",
     "exception": false,
     "start_time": "2025-05-24T05:04:27.108300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = {'row_id': []}\n",
    "for species_code in class_labels:\n",
    "    pred[species_code] = []\n",
    "    \n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    _ = list(executor.map(prediction, file_list))\n",
    "end_t = time.time()\n",
    "\n",
    "if debug == True:\n",
    "    print(700*(end_t - start)/60/debug_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab9294a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:27.144247Z",
     "iopub.status.busy": "2025-05-24T05:04:27.143895Z",
     "iopub.status.idle": "2025-05-24T05:04:27.171523Z",
     "shell.execute_reply": "2025-05-24T05:04:27.170571Z"
    },
    "papermill": {
     "duration": 0.038863,
     "end_time": "2025-05-24T05:04:27.173259",
     "exception": false,
     "start_time": "2025-05-24T05:04:27.134396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, 1139490, 1192948, 1194042, 126247, 1346504, 134933, 135045, 1462711, 1462737, 1564122, 21038, 21116, 21211, 22333, 22973, 22976, 24272, 24292, 24322, 41663, 41778, 41970, 42007, 42087, 42113, 46010, 47067, 476537, 476538, 48124, 50186, 517119, 523060, 528041, 52884, 548639, 555086, 555142, 566513, 64862, 65336, 65344, 65349, 65373, 65419, 65448, 65547, 65962, 66016, 66531, 66578, 66893, 67082, 67252, 714022, 715170, 787625, 81930, 868458, 963335, amakin1, amekes, ampkin1, anhing, babwar, bafibi1, banana, baymac, bbwduc, bicwre1, bkcdon, bkmtou1, blbgra1, blbwre1, blcant4, blchaw1, blcjay1, blctit1, blhpar1, blkvul, bobfly1, bobher1, brtpar1, bubcur1, bubwre1, bucmot3, bugtan, butsal1, cargra1, cattyr, chbant1, chfmac1, cinbec1, cocher1, cocwoo1, colara1, colcha1, compau, compot1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 207 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(pred, columns = ['row_id'] + class_labels) \n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05a74115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:27.192883Z",
     "iopub.status.busy": "2025-05-24T05:04:27.192504Z",
     "iopub.status.idle": "2025-05-24T05:04:27.224290Z",
     "shell.execute_reply": "2025-05-24T05:04:27.223343Z"
    },
    "papermill": {
     "duration": 0.043505,
     "end_time": "2025-05-24T05:04:27.226116",
     "exception": false,
     "start_time": "2025-05-24T05:04:27.182611",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"submission.csv\", index=False)    \n",
    "\n",
    "sub = pd.read_csv('submission.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "\n",
    "if debug:\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d74df8",
   "metadata": {
    "papermill": {
     "duration": 0.008949,
     "end_time": "2025-05-24T05:04:27.244577",
     "exception": false,
     "start_time": "2025-05-24T05:04:27.235628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "《《《Finaly Blending》》》\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e35e37ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:27.264675Z",
     "iopub.status.busy": "2025-05-24T05:04:27.264332Z",
     "iopub.status.idle": "2025-05-24T05:04:27.268706Z",
     "shell.execute_reply": "2025-05-24T05:04:27.267628Z"
    },
    "papermill": {
     "duration": 0.016394,
     "end_time": "2025-05-24T05:04:27.270466",
     "exception": false,
     "start_time": "2025-05-24T05:04:27.254072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------- #\n",
    "# [IMPORTANT]\n",
    "# * Blending Weight\n",
    "# ------------------------------------------ #\n",
    "sub_w=[0.4,0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "450739e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:04:27.290738Z",
     "iopub.status.busy": "2025-05-24T05:04:27.290410Z",
     "iopub.status.idle": "2025-05-24T05:04:29.169852Z",
     "shell.execute_reply": "2025-05-24T05:04:29.168653Z"
    },
    "papermill": {
     "duration": 1.892197,
     "end_time": "2025-05-24T05:04:29.171913",
     "exception": false,
     "start_time": "2025-05-24T05:04:27.279716",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_TARGETs = sorted(os.listdir('/kaggle/input/birdclef-2025/train_audio/'))\n",
    "list_targets_0 = [f'{TARGET} 0' for TARGET in list_TARGETs]\n",
    "list_targets_1 = [f'{TARGET} 1' for TARGET in list_TARGETs]\n",
    "\n",
    "df0 = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
    "df1 = pd.read_csv(\"/kaggle/working/submission1.csv\")\n",
    "\n",
    "df0 = df0.rename(columns={TARGET : f'{TARGET} 0' for TARGET in list_TARGETs})\n",
    "df1 = df1.rename(columns={TARGET : f'{TARGET} 1' for TARGET in list_TARGETs})\n",
    "\n",
    "dfs = pd.merge(df0,df1,on=['row_id'])\n",
    "\n",
    "for i in range(len(list_TARGETs)):\n",
    "    dfs[list_TARGETs[i]] = dfs[list_targets_0[i]]*sub_w[0] + sub_w[1]*dfs[list_targets_1[i]]\n",
    "             \n",
    "for col0,col1 in zip(list_targets_0, list_targets_1):\n",
    "    del dfs[col0]\n",
    "    del dfs[col1]\n",
    "    \n",
    "    \n",
    "dfs.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7430593,
     "sourceId": 11828260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7457365,
     "sourceId": 11867185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.184659,
   "end_time": "2025-05-24T05:04:31.821405",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-24T05:03:28.636746",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
